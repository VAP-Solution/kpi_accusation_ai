{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"machine_shape":"hm","gpuType":"A100"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"84f6418b77c5474b8adeead02eedc065":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_e2f73c0533544e9a8eefcec8fb883cfa","IPY_MODEL_1abecfb90c1243fa80f0ac2aaec579b8","IPY_MODEL_39dc118625b3458e84e986e404ff8f0e"],"layout":"IPY_MODEL_9aa5fa2a3e854af8abd1fae2d60a4fc7"}},"e2f73c0533544e9a8eefcec8fb883cfa":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_89ed128c10944e9ba919def4a4fc142a","placeholder":"​","style":"IPY_MODEL_2f670b78b7434e6d9a0d5e5a2ff16bfa","value":"Loading checkpoint shards: 100%"}},"1abecfb90c1243fa80f0ac2aaec579b8":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_99e01b114aca4c409be4e466d3297a26","max":2,"min":0,"orientation":"horizontal","style":"IPY_MODEL_8c3ae76df797411e900eadd70a02b7df","value":2}},"39dc118625b3458e84e986e404ff8f0e":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_8dc7660d7ac34ee0a68d5f5446dfd067","placeholder":"​","style":"IPY_MODEL_d9ea530536a7455c94b2491e6963fa31","value":" 2/2 [00:01&lt;00:00,  1.51it/s]"}},"9aa5fa2a3e854af8abd1fae2d60a4fc7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"89ed128c10944e9ba919def4a4fc142a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2f670b78b7434e6d9a0d5e5a2ff16bfa":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"99e01b114aca4c409be4e466d3297a26":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8c3ae76df797411e900eadd70a02b7df":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"8dc7660d7ac34ee0a68d5f5446dfd067":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d9ea530536a7455c94b2491e6963fa31":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"aada607319ba4409a67888bd28003837":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_34ef9f8c4ce448daa974263c7790f8f9","IPY_MODEL_7a99c937e4e7408e97377f76c643d61e","IPY_MODEL_08943ee3b41a41abb2d3c94f040c6ce9"],"layout":"IPY_MODEL_86d1bab8a2834687b0bf876103089400"}},"34ef9f8c4ce448daa974263c7790f8f9":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_cceb4c0fada4433c9077d5a0da6da2e7","placeholder":"​","style":"IPY_MODEL_c6dd5583ccb3453ca72f30d0fae99671","value":"Map: 100%"}},"7a99c937e4e7408e97377f76c643d61e":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_ab0582f96072423ba62b9317ff55b0ef","max":567,"min":0,"orientation":"horizontal","style":"IPY_MODEL_57dcb3c479c848f3a7376d2b6c888dcd","value":567}},"08943ee3b41a41abb2d3c94f040c6ce9":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_af1a8dc1d8134e93ab1a56a19e2e62be","placeholder":"​","style":"IPY_MODEL_1bc6047512fd43f0ada8a8998192d183","value":" 567/567 [00:00&lt;00:00, 4029.81 examples/s]"}},"86d1bab8a2834687b0bf876103089400":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"cceb4c0fada4433c9077d5a0da6da2e7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c6dd5583ccb3453ca72f30d0fae99671":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"ab0582f96072423ba62b9317ff55b0ef":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"57dcb3c479c848f3a7376d2b6c888dcd":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"af1a8dc1d8134e93ab1a56a19e2e62be":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1bc6047512fd43f0ada8a8998192d183":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"030e45549b91424c83ccd5a9927e7dc3":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_1dff2a510f0c472d9c07dd968af04f40","IPY_MODEL_64ab267c2b3c4984bad712af38b1fb02","IPY_MODEL_e376aa34ff184cecb8188c55aa8383b6"],"layout":"IPY_MODEL_892fd574afcd4276be80aff9caad6282"}},"1dff2a510f0c472d9c07dd968af04f40":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_49b103ec920c457fa74b56eb92098dc5","placeholder":"​","style":"IPY_MODEL_a1fe7433534c4d9bb9aba36fe8c29472","value":"Map: 100%"}},"64ab267c2b3c4984bad712af38b1fb02":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_fed3c0e49477408d98e6c11971877902","max":142,"min":0,"orientation":"horizontal","style":"IPY_MODEL_c4037bd9188d45eb93521e563a9136ae","value":142}},"e376aa34ff184cecb8188c55aa8383b6":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_20bee62e15364e208a133a34e549a181","placeholder":"​","style":"IPY_MODEL_74fa34c625bd4acebeb23c2cff3b4be7","value":" 142/142 [00:00&lt;00:00, 3033.73 examples/s]"}},"892fd574afcd4276be80aff9caad6282":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"49b103ec920c457fa74b56eb92098dc5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a1fe7433534c4d9bb9aba36fe8c29472":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"fed3c0e49477408d98e6c11971877902":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c4037bd9188d45eb93521e563a9136ae":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"20bee62e15364e208a133a34e549a181":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"74fa34c625bd4acebeb23c2cff3b4be7":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"4b58694f21c34878bad6ea66a95dbcfd":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_35c77e5f23054de9b2c04fa4439e460a","IPY_MODEL_5e1f87a266d64f0f96a7fcad0a607e2d","IPY_MODEL_150f1a4dc4144083bf8188f10d6ca0f6"],"layout":"IPY_MODEL_e8af0fa9a35e4d10bcf5ee6364114bde"}},"35c77e5f23054de9b2c04fa4439e460a":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_986a59f36731400ba8b690e1bf6e3d25","placeholder":"​","style":"IPY_MODEL_cda79a3250d54546b65d3eb666c6e209","value":"Map: 100%"}},"5e1f87a266d64f0f96a7fcad0a607e2d":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_b76bf3c174644fe1b0192c3ad06900a1","max":178,"min":0,"orientation":"horizontal","style":"IPY_MODEL_3648361cc89e475da9537e74002c5cdc","value":178}},"150f1a4dc4144083bf8188f10d6ca0f6":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_2361e53287834269a1a870745ced7cab","placeholder":"​","style":"IPY_MODEL_3bc72b8d0fad4efaa150bbefb233a3d3","value":" 178/178 [00:00&lt;00:00, 3203.96 examples/s]"}},"e8af0fa9a35e4d10bcf5ee6364114bde":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"986a59f36731400ba8b690e1bf6e3d25":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"cda79a3250d54546b65d3eb666c6e209":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"b76bf3c174644fe1b0192c3ad06900a1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3648361cc89e475da9537e74002c5cdc":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"2361e53287834269a1a870745ced7cab":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3bc72b8d0fad4efaa150bbefb233a3d3":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"cells":[{"cell_type":"code","source":["# Adjusted Fine-tuning Code for Numeric Classification\n","from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pGlC3mUww4LT","executionInfo":{"status":"ok","timestamp":1755832430308,"user_tz":-420,"elapsed":35293,"user":{"displayName":"VAP Solution","userId":"02613841423815522130"}},"outputId":"3bf5fae3-075b-4583-fd57-6131e7fe589a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":602},"id":"dgm_iYhcwwf8","executionInfo":{"status":"error","timestamp":1756136888210,"user_tz":-420,"elapsed":77477,"user":{"displayName":"emmanuel Mayowa","userId":"14227797463057961975"}},"outputId":"7f6d30e0-3ca8-4cfc-d7f3-e2d9580072d7"},"outputs":[{"output_type":"stream","name":"stdout","text":["Installing required packages...\n","All packages installed successfully!\n","================================================================================\n","IMPROVED NACC COMPLAINT CLASSIFICATION FINE-TUNING\n","================================================================================\n","\n","1. Setting up category mapping...\n","Category mapping saved to: /content/drive/MyDrive/Government/KPI/NACC AI Project/[00] NACC LLM/Paper Revision/finetune_new/category_mapping.json\n","\n","2. Loading and preprocessing data...\n","Error loading dataset: [Errno 2] No such file or directory: '/content/drive/MyDrive/Government/KPI/NACC AI Project/[00] NACC LLM/Paper Revision/data/Trainset.csv'\n"]},{"output_type":"error","ename":"FileNotFoundError","evalue":"[Errno 2] No such file or directory: '/content/drive/MyDrive/Government/KPI/NACC AI Project/[00] NACC LLM/Paper Revision/data/Trainset.csv'","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipython-input-4166168359.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[0;31m# Load data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 136\u001b[0;31m     \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDATASET_PATH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    137\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Loaded dataset with {len(df)} samples\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Columns: {list(df.columns)}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1619\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1620\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1622\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1879\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1880\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1881\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    880\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    881\u001b[0m             \u001b[0;31m# Binary mode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 882\u001b[0;31m             \u001b[0mhandle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    883\u001b[0m         \u001b[0mhandles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    884\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/drive/MyDrive/Government/KPI/NACC AI Project/[00] NACC LLM/Paper Revision/data/Trainset.csv'"]}],"source":["# Improved Fine-tuning Code for NACC Complaint Classification\n","\n","# Install required packages\n","import subprocess\n","import sys\n","\n","def install_package(package):\n","    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package])\n","\n","# Install required packages\n","print(\"Installing required packages...\")\n","try:\n","    install_package(\"bitsandbytes\")\n","    install_package(\"peft\")\n","    install_package(\"accelerate\")\n","    install_package(\"transformers>=4.35.0\")\n","    install_package(\"datasets\")\n","    install_package(\"scikit-learn\")\n","    install_package(\"matplotlib\")\n","    install_package(\"seaborn\")\n","    print(\"All packages installed successfully!\")\n","except Exception as e:\n","    print(f\"Error installing packages: {e}\")\n","\n","import os\n","import json\n","import pandas as pd\n","import numpy as np\n","import torch\n","from datetime import datetime\n","from sklearn.model_selection import train_test_split, StratifiedKFold\n","from sklearn.utils.class_weight import compute_class_weight\n","from sklearn.metrics import accuracy_score, f1_score, precision_recall_fscore_support, classification_report, confusion_matrix\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","\n","from datasets import Dataset, DatasetDict\n","from transformers import (\n","    AutoTokenizer,\n","    AutoModelForSequenceClassification,\n","    BitsAndBytesConfig,\n","    TrainingArguments,\n","    Trainer,\n","    EarlyStoppingCallback\n",")\n","from peft import get_peft_model, LoraConfig, TaskType\n","import warnings\n","warnings.filterwarnings('ignore')\n","\n","# =============================================================================\n","# CONFIGURATION\n","# =============================================================================\n","\n","# Paths\n","DATASET_PATH = \"/content/drive/MyDrive/Government/KPI/NACC AI Project/[00] NACC LLM/Paper Revision/data/Trainset.csv\"\n","OUTPUT_DIR = \"/content/drive/MyDrive/Government/KPI/NACC AI Project/[00] NACC LLM/Paper Revision/finetune_new\"\n","\n","# Model configuration\n","MODEL_ID = \"scb10x/llama3.2-typhoon2-3b-instruct\"\n","MAX_LENGTH = 512  # Shorter sequences for efficiency\n","BATCH_SIZE = 8    # Larger batch size\n","GRADIENT_ACCUMULATION = 2\n","NUM_EPOCHS = 5    # More epochs for gradual learning\n","LEARNING_RATE = 1e-5  # Even lower learning rate\n","LORA_R = 8        # Smaller rank to reduce overfitting\n","LORA_ALPHA = 16   # Adjusted alpha\n","LORA_DROPOUT = 0.2  # Higher dropout\n","WEIGHT_DECAY = 0.1  # Strong regularization\n","\n","# Data augmentation parameters\n","AUG_RATIO = 0.3   # 30% of data will be augmented\n","USE_AUGMENTATION = True\n","USE_CROSS_VALIDATION = True\n","CV_FOLDS = 5\n","\n","# Create output directory\n","os.makedirs(OUTPUT_DIR, exist_ok=True)\n","\n","print(\"=\" * 80)\n","print(\"IMPROVED NACC COMPLAINT CLASSIFICATION FINE-TUNING\")\n","print(\"=\" * 80)\n","\n","# =============================================================================\n","# 1. CATEGORY MAPPING\n","# =============================================================================\n","\n","print(\"\\n1. Setting up category mapping...\")\n","\n","CATEGORY_MAPPING = {\n","    \"ปฏิบัติหรือละเว้นการปฏิบัติหน้าที่โดยมิชอบ\": 0,\n","    \"ทุจริตในการจัดทำงบประมาณ/โครงการ/เบิกจ่ายเงินในโครงการเป็นเท็จ\": 1,\n","    \"จัดซื้อจัดจ้าง\": 2,\n","    \"ออกเอกสารสิทธิที่ดิน\": 3,\n","    \"ยักยอก/เบียดบังเงินหรือทรัพย์สินของราชการ\": 4,\n","    \"การบริหารงานบุคคล (การบรรจุ/แต่งตั้ง/เลื่อนตำแหน่ง/โยกย้าย/ลงโทษวินัย)\": 5,\n","    \"ร่ำรวยผิดปกติ\": 6,\n","    \"เรียกรับสินบน\": 7,\n","    \"การขัดกันระหว่างประโยชน์ส่วนบุคคลกับประโยชน์ส่วนรวม\": 8,\n","    \"ก่าเกื้นจริยธรรม\": 9\n","}\n","\n","NUMERIC_TO_CATEGORY = {v: k for k, v in CATEGORY_MAPPING.items()}\n","\n","ENGLISH_NAMES = {\n","    0: \"Abuse of Power\",\n","    1: \"Budget/Project Fraud\",\n","    2: \"Procurement Fraud\",\n","    3: \"Fraudulent Land Title\",\n","    4: \"Embezzlement\",\n","    5: \"Personnel Misconduct\",\n","    6: \"Unusual Wealth\",\n","    7: \"Bribery\",\n","    8: \"Conflict of Interest\",\n","    9: \"Ethical Misconduct\"\n","}\n","\n","# Save mapping\n","mapping_file = os.path.join(OUTPUT_DIR, \"category_mapping.json\")\n","with open(mapping_file, 'w', encoding='utf-8') as f:\n","    json.dump({\n","        \"thai_to_numeric\": CATEGORY_MAPPING,\n","        \"numeric_to_thai\": NUMERIC_TO_CATEGORY,\n","        \"english_names\": ENGLISH_NAMES\n","    }, f, ensure_ascii=False, indent=2)\n","\n","print(f\"Category mapping saved to: {mapping_file}\")\n","\n","# =============================================================================\n","# 2. DATA LOADING AND PREPROCESSING\n","# =============================================================================\n","\n","print(\"\\n2. Loading and preprocessing data...\")\n","\n","# Load data\n","try:\n","    df = pd.read_csv(DATASET_PATH, encoding='utf-8')\n","    print(f\"Loaded dataset with {len(df)} samples\")\n","    print(f\"Columns: {list(df.columns)}\")\n","except Exception as e:\n","    print(f\"Error loading dataset: {e}\")\n","    raise\n","\n","# Check data structure\n","print(\"\\nData structure:\")\n","print(df.head())\n","print(f\"\\nDataset shape: {df.shape}\")\n","\n","# Map categories to numeric\n","df['category_numeric'] = df['category'].map(CATEGORY_MAPPING)\n","\n","# Check for unmapped categories\n","unmapped = df[df['category_numeric'].isna()]\n","if len(unmapped) > 0:\n","    print(f\"Warning: {len(unmapped)} unmapped categories found:\")\n","    print(unmapped['category'].unique())\n","    df = df.dropna(subset=['category_numeric'])\n","\n","# Convert to int\n","df['category_numeric'] = df['category_numeric'].astype(int)\n","\n","print(f\"Final dataset size: {len(df)}\")\n","\n","# Check class distribution\n","class_dist = df['category_numeric'].value_counts().sort_index()\n","print(\"\\nClass distribution:\")\n","for cat_id, count in class_dist.items():\n","    english_name = ENGLISH_NAMES[cat_id]\n","    percentage = (count / len(df)) * 100\n","    print(f\"{cat_id}: {english_name} - {count} samples ({percentage:.1f}%)\")\n","\n","# =============================================================================\n","# 3. DATA AUGMENTATION FOR SMALL DATASET\n","# =============================================================================\n","\n","print(\"\\n3. Implementing data augmentation...\")\n","\n","def simple_back_translation_augment(text):\n","    \"\"\"Simple augmentation through word shuffling and synonym replacement\"\"\"\n","    import random\n","\n","    # Simple word shuffling (keeping sentence structure)\n","    sentences = text.split('.')\n","    augmented_sentences = []\n","\n","    for sentence in sentences:\n","        words = sentence.strip().split()\n","        if len(words) > 3:\n","            # Shuffle middle words occasionally\n","            if random.random() < 0.3:\n","                middle = words[1:-1]\n","                random.shuffle(middle)\n","                words = [words[0]] + middle + [words[-1]]\n","        augmented_sentences.append(' '.join(words))\n","\n","    return '.'.join(augmented_sentences)\n","\n","def character_level_augment(text):\n","    \"\"\"Character-level augmentation for Thai text\"\"\"\n","    import random\n","\n","    chars = list(text)\n","    if len(chars) > 10:\n","        # Random character swapping (very conservative)\n","        if random.random() < 0.1:\n","            idx1, idx2 = random.sample(range(1, len(chars)-1), 2)\n","            chars[idx1], chars[idx2] = chars[idx2], chars[idx1]\n","\n","    return ''.join(chars)\n","\n","def augment_text(text, method='shuffle'):\n","    \"\"\"Apply text augmentation\"\"\"\n","    if method == 'shuffle':\n","        return simple_back_translation_augment(text)\n","    elif method == 'char':\n","        return character_level_augment(text)\n","    else:\n","        return text\n","\n","def create_augmented_data(X, y, augmentation_ratio=0.3):\n","    \"\"\"Create augmented dataset\"\"\"\n","    print(f\"Creating augmented data with ratio: {augmentation_ratio}\")\n","\n","    # Calculate samples to augment per class\n","    unique_classes, class_counts = np.unique(y, return_counts=True)\n","    min_samples = min(class_counts)\n","\n","    X_aug = []\n","    y_aug = []\n","\n","    for class_id in unique_classes:\n","        class_indices = np.where(y == class_id)[0]\n","        class_texts = X[class_indices]\n","\n","        # Calculate how many samples to augment for this class\n","        current_count = len(class_indices)\n","        target_augment = int(current_count * augmentation_ratio)\n","\n","        # Randomly select samples to augment\n","        aug_indices = np.random.choice(len(class_texts), size=target_augment, replace=True)\n","\n","        for idx in aug_indices:\n","            original_text = class_texts[idx]\n","\n","            # Apply different augmentation methods\n","            aug_method = np.random.choice(['shuffle', 'char'], p=[0.7, 0.3])\n","            augmented_text = augment_text(original_text, method=aug_method)\n","\n","            # Only add if augmentation actually changed the text\n","            if augmented_text != original_text:\n","                X_aug.append(augmented_text)\n","                y_aug.append(class_id)\n","\n","    print(f\"Generated {len(X_aug)} augmented samples\")\n","    return np.array(X_aug), np.array(y_aug)\n","\n","# Apply data augmentation if enabled\n","if USE_AUGMENTATION and len(df) < 1000:  # Only for small datasets\n","    print(\"Dataset is small, applying data augmentation...\")\n","    X_aug, y_aug = create_augmented_data(X, y, AUG_RATIO)\n","\n","    # Combine original and augmented data\n","    X_combined = np.concatenate([X, X_aug])\n","    y_combined = np.concatenate([y, y_aug])\n","\n","    print(f\"Original dataset: {len(X)} samples\")\n","    print(f\"Augmented dataset: {len(X_combined)} samples\")\n","\n","    # Use combined data for training\n","    X, y = X_combined, y_combined\n","else:\n","    print(\"Skipping augmentation (dataset size sufficient or disabled)\")\n","\n","# =============================================================================\n","# 4. STRATIFIED TRAIN-TEST SPLIT WITH CROSS-VALIDATION\n","# =============================================================================\n","\n","print(\"\\n4. Creating stratified splits...\")\n","\n","# First, create a hold-out test set (20%)\n","X_temp, X_test, y_temp, y_test = train_test_split(\n","    X, y,\n","    test_size=0.2,\n","    stratify=y,\n","    random_state=42\n",")\n","\n","# Then split remaining data for train/validation\n","X_train, X_val, y_train, y_val = train_test_split(\n","    X_temp, y_temp,\n","    test_size=0.2,  # 20% of remaining 80% = 16% of total\n","    stratify=y_temp,\n","    random_state=42\n",")\n","\n","print(f\"Train set: {len(X_train)} samples ({len(X_train)/len(X)*100:.1f}%)\")\n","print(f\"Validation set: {len(X_val)} samples ({len(X_val)/len(X)*100:.1f}%)\")\n","print(f\"Test set: {len(X_test)} samples ({len(X_test)/len(X)*100:.1f}%)\")\n","\n","# Check distributions\n","train_dist = pd.Series(y_train).value_counts().sort_index()\n","val_dist = pd.Series(y_val).value_counts().sort_index()\n","test_dist = pd.Series(y_test).value_counts().sort_index()\n","\n","print(\"\\nClass distributions:\")\n","for cat_id in range(10):\n","    train_count = train_dist.get(cat_id, 0)\n","    val_count = val_dist.get(cat_id, 0)\n","    test_count = test_dist.get(cat_id, 0)\n","    print(f\"  Class {cat_id}: Train={train_count}, Val={val_count}, Test={test_count}\")\n","\n","# =============================================================================\n","# 5. COMPUTE CLASS WEIGHTS FOR IMBALANCED DATA\n","# =============================================================================\n","\n","print(\"\\n5. Computing class weights...\")\n","\n","class_weights = compute_class_weight(\n","    'balanced',\n","    classes=np.unique(y_train),\n","    y=y_train\n",")\n","\n","class_weight_dict = {i: weight for i, weight in enumerate(class_weights)}\n","print(\"Class weights:\")\n","for cat_id, weight in class_weight_dict.items():\n","    if cat_id < len(ENGLISH_NAMES):\n","        print(f\"  {cat_id} ({ENGLISH_NAMES[cat_id]}): {weight:.3f}\")\n","\n","# =============================================================================\n","# 6. CREATE DATASETS WITH VALIDATION\n","# =============================================================================\n","\n","print(\"\\n6. Creating datasets...\")\n","\n","# Create datasets\n","train_dataset = Dataset.from_dict({\n","    'text': X_train,\n","    'labels': y_train\n","})\n","\n","val_dataset = Dataset.from_dict({\n","    'text': X_val,\n","    'labels': y_val\n","})\n","\n","test_dataset = Dataset.from_dict({\n","    'text': X_test,\n","    'labels': y_test\n","})\n","\n","# Create DatasetDict\n","datasets = DatasetDict({\n","    'train': train_dataset,\n","    'validation': val_dataset,\n","    'test': test_dataset\n","})\n","\n","print(\"Datasets created successfully\")\n","\n","# =============================================================================\n","# 6. MODEL AND TOKENIZER SETUP\n","# =============================================================================\n","\n","print(\"\\n6. Setting up model and tokenizer...\")\n","\n","# Load tokenizer\n","tokenizer = AutoTokenizer.from_pretrained(MODEL_ID)\n","if tokenizer.pad_token is None:\n","    tokenizer.pad_token = tokenizer.eos_token\n","\n","print(\"Tokenizer loaded\")\n","\n","# Setup quantization config with fallback\n","try:\n","    quantization_config = BitsAndBytesConfig(\n","        load_in_4bit=True,\n","        bnb_4bit_use_double_quant=True,\n","        bnb_4bit_compute_dtype=torch.bfloat16,\n","        bnb_4bit_quant_type=\"nf4\"\n","    )\n","    use_quantization = True\n","    print(\"4-bit quantization enabled\")\n","except Exception as e:\n","    print(f\"Quantization not available: {e}\")\n","    print(\"Loading model without quantization...\")\n","    quantization_config = None\n","    use_quantization = False\n","\n","# Load model for classification\n","model_kwargs = {\n","    \"num_labels\": 10,  # Number of categories\n","    \"device_map\": \"auto\",\n","    \"trust_remote_code\": True\n","}\n","\n","if use_quantization and quantization_config is not None:\n","    model_kwargs[\"quantization_config\"] = quantization_config\n","\n","try:\n","    model = AutoModelForSequenceClassification.from_pretrained(\n","        MODEL_ID,\n","        **model_kwargs\n","    )\n","except Exception as e:\n","    print(f\"Error loading model with quantization: {e}\")\n","    print(\"Retrying without quantization...\")\n","    # Fallback without quantization\n","    model = AutoModelForSequenceClassification.from_pretrained(\n","        MODEL_ID,\n","        num_labels=10,\n","        trust_remote_code=True\n","    )\n","\n","print(\"Base model loaded\")\n","\n","# Apply LoRA with stronger regularization for small datasets\n","peft_config = LoraConfig(\n","    r=LORA_R,           # Smaller rank reduces parameters\n","    lora_alpha=LORA_ALPHA,\n","    lora_dropout=LORA_DROPOUT,  # Higher dropout\n","    bias=\"none\",\n","    task_type=TaskType.SEQ_CLS,\n","    target_modules=[\"q_proj\", \"v_proj\", \"k_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"]\n",")\n","\n","model = get_peft_model(model, peft_config)\n","\n","# Freeze more layers for small dataset\n","if len(X_train) < 1000:\n","    print(\"Small dataset detected. Applying additional regularization...\")\n","\n","    # Freeze embedding layers\n","    for name, param in model.named_parameters():\n","        if 'embed' in name.lower():\n","            param.requires_grad = False\n","\n","    # Freeze some transformer layers (freeze bottom layers, train top layers)\n","    total_layers = len([n for n, p in model.named_parameters() if 'layers.' in n and 'weight' in n])\n","    layers_to_freeze = total_layers // 3  # Freeze bottom 1/3 of layers\n","\n","    for name, param in model.named_parameters():\n","        if 'layers.' in name:\n","            layer_num = int(name.split('layers.')[1].split('.')[0])\n","            if layer_num < layers_to_freeze:\n","                param.requires_grad = False\n","\n","model.print_trainable_parameters()\n","\n","# =============================================================================\n","# 7. TOKENIZATION\n","# =============================================================================\n","\n","print(\"\\n7. Tokenizing data...\")\n","\n","def tokenize_function(examples):\n","    \"\"\"Tokenize complaints for classification\"\"\"\n","    return tokenizer(\n","        examples['text'],\n","        truncation=True,\n","        padding=True,\n","        max_length=MAX_LENGTH,\n","        return_tensors=None\n","    )\n","\n","# Tokenize datasets\n","tokenized_datasets = datasets.map(\n","    tokenize_function,\n","    batched=True,\n","    remove_columns=['text']\n",")\n","\n","print(\"Tokenization completed\")\n","\n","# =============================================================================\n","# 8. EVALUATION METRICS\n","# =============================================================================\n","\n","def compute_metrics(eval_pred):\n","    \"\"\"Compute comprehensive evaluation metrics\"\"\"\n","    predictions, labels = eval_pred\n","    predictions = np.argmax(predictions, axis=1)\n","\n","    # Basic metrics\n","    accuracy = accuracy_score(labels, predictions)\n","\n","    # Per-class metrics\n","    precision, recall, f1, support = precision_recall_fscore_support(\n","        labels, predictions, average=None, zero_division=0\n","    )\n","\n","    # Macro and weighted averages\n","    f1_macro = f1_score(labels, predictions, average='macro', zero_division=0)\n","    f1_weighted = f1_score(labels, predictions, average='weighted', zero_division=0)\n","\n","    return {\n","        'accuracy': accuracy,\n","        'f1_macro': f1_macro,\n","        'f1_weighted': f1_weighted,\n","        'precision_macro': np.mean(precision),\n","        'recall_macro': np.mean(recall)\n","    }\n","\n","# =============================================================================\n","# 9. TRAINING SETUP\n","# =============================================================================\n","\n","print(\"\\n9. Setting up training...\")\n","\n","# Training arguments\n","training_args = TrainingArguments(\n","    output_dir=os.path.join(OUTPUT_DIR, \"checkpoints\"),\n","    num_train_epochs=NUM_EPOCHS,\n","    per_device_train_batch_size=BATCH_SIZE,\n","    per_device_eval_batch_size=BATCH_SIZE,\n","    gradient_accumulation_steps=GRADIENT_ACCUMULATION,\n","    learning_rate=LEARNING_RATE,\n","    weight_decay=0.01,\n","    warmup_ratio=0.1,\n","    logging_steps=10,\n","    evaluation_strategy=\"epoch\",\n","    save_strategy=\"epoch\",\n","    save_total_limit=3,\n","    load_best_model_at_end=True,\n","    metric_for_best_model=\"eval_f1_macro\",\n","    greater_is_better=True,\n","    report_to=\"none\",\n","    dataloader_pin_memory=False,\n","    bf16=torch.cuda.is_available(),\n","    optim=\"paged_adamw_8bit\",\n","    run_name=f\"nacc-classification-{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",")\n","\n","# Custom trainer with advanced regularization for small datasets\n","class AdvancedRegularizedTrainer(Trainer):\n","    def __init__(self, mixup_alpha=0.2, label_smoothing=0.1, **kwargs):\n","        super().__init__(**kwargs)\n","        self.mixup_alpha = mixup_alpha\n","        self.label_smoothing = label_smoothing\n","\n","    def compute_loss(self, model, inputs, return_outputs=False):\n","        labels = inputs.get(\"labels\")\n","        outputs = model(**inputs)\n","        logits = outputs.get('logits')\n","\n","        # Apply class weights\n","        weight_tensor = torch.tensor(list(class_weight_dict.values()),\n","                                   dtype=torch.float32, device=labels.device)\n","\n","        # Label smoothing for regularization\n","        if self.label_smoothing > 0:\n","            loss_fct = torch.nn.CrossEntropyLoss(\n","                weight=weight_tensor,\n","                label_smoothing=self.label_smoothing\n","            )\n","        else:\n","            loss_fct = torch.nn.CrossEntropyLoss(weight=weight_tensor)\n","\n","        loss = loss_fct(logits, labels)\n","\n","        # Add L2 regularization to LoRA parameters\n","        l2_reg = 0\n","        for name, param in model.named_parameters():\n","            if 'lora_' in name and param.requires_grad:\n","                l2_reg += torch.norm(param, p=2)\n","\n","        loss = loss + 0.01 * l2_reg  # L2 regularization coefficient\n","\n","        return (loss, outputs) if return_outputs else loss\n","\n","    def training_step(self, model, inputs):\n","        \"\"\"Custom training step with mixup augmentation\"\"\"\n","        model.train()\n","        inputs = self._prepare_inputs(inputs)\n","\n","        # Apply mixup with small probability for regularization\n","        if np.random.random() < 0.1 and self.mixup_alpha > 0:\n","            inputs = self._apply_mixup(inputs)\n","\n","        with self.compute_loss_context_manager():\n","            loss = self.compute_loss(model, inputs)\n","\n","        if self.args.n_gpu > 1:\n","            loss = loss.mean()\n","\n","        if self.args.gradient_accumulation_steps > 1:\n","            loss = loss / self.args.gradient_accumulation_steps\n","\n","        loss.backward()\n","        return loss.detach()\n","\n","    def _apply_mixup(self, inputs):\n","        \"\"\"Apply mixup augmentation\"\"\"\n","        input_ids = inputs['input_ids']\n","        attention_mask = inputs['attention_mask']\n","        labels = inputs['labels']\n","\n","        batch_size = input_ids.size(0)\n","\n","        # Generate random indices for mixing\n","        indices = torch.randperm(batch_size)\n","        lam = np.random.beta(self.mixup_alpha, self.mixup_alpha)\n","\n","        # Mix inputs (this is simplified - real mixup for text is complex)\n","        # For simplicity, we'll just mix the labels and apply stronger regularization\n","        mixed_labels = lam * labels + (1 - lam) * labels[indices]\n","\n","        inputs['labels'] = mixed_labels.long()  # Convert back to long for classification\n","        return inputs\n","\n","# Enhanced training arguments for small datasets\n","training_args = TrainingArguments(\n","    output_dir=os.path.join(OUTPUT_DIR, \"checkpoints\"),\n","    num_train_epochs=NUM_EPOCHS,\n","    per_device_train_batch_size=BATCH_SIZE,\n","    per_device_eval_batch_size=BATCH_SIZE,\n","    gradient_accumulation_steps=GRADIENT_ACCUMULATION,\n","    learning_rate=LEARNING_RATE,\n","    weight_decay=WEIGHT_DECAY,  # Strong weight decay\n","    warmup_ratio=0.1,\n","    lr_scheduler_type=\"cosine\",  # Cosine annealing\n","    logging_steps=10,\n","    evaluation_strategy=\"steps\",  # More frequent evaluation\n","    eval_steps=50,  # Evaluate every 50 steps\n","    save_strategy=\"steps\",\n","    save_steps=50,\n","    save_total_limit=5,  # Keep more checkpoints\n","    load_best_model_at_end=True,\n","    metric_for_best_model=\"eval_f1_macro\",\n","    greater_is_better=True,\n","    report_to=\"none\",\n","    dataloader_pin_memory=False,\n","    bf16=torch.cuda.is_available(),\n","    fp16=not torch.cuda.is_available() or not torch.cuda.get_device_capability()[0] >= 8,\n","    optim=\"adamw_torch\",\n","    adam_beta1=0.9,\n","    adam_beta2=0.999,\n","    adam_epsilon=1e-8,\n","    max_grad_norm=1.0,  # Gradient clipping\n","    dataloader_drop_last=True,  # Drop incomplete batches\n","    run_name=f\"nacc-classification-{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",")\n","\n","# Create trainer with advanced regularization\n","trainer = AdvancedRegularizedTrainer(\n","    model=model,\n","    args=training_args,\n","    train_dataset=tokenized_datasets[\"train\"],\n","    eval_dataset=tokenized_datasets[\"validation\"],  # Use validation set\n","    compute_metrics=compute_metrics,\n","    mixup_alpha=0.2 if len(X_train) < 1000 else 0.0,  # Enable mixup for small datasets\n","    label_smoothing=0.1,  # Label smoothing for regularization\n","    callbacks=[\n","        EarlyStoppingCallback(early_stopping_patience=5, early_stopping_threshold=0.001)\n","    ]\n",")\n","\n","print(\"Trainer created successfully\")\n","\n","# =============================================================================\n","# 10. TRAINING\n","# =============================================================================\n","\n","print(\"\\n10. Starting training...\")\n","print(\"=\" * 50)\n","\n","# Train the model\n","training_result = trainer.train()\n","\n","print(\"Training completed!\")\n","print(f\"Final training loss: {training_result.training_loss:.4f}\")\n","\n","# =============================================================================\n","# 11. EVALUATION WITH FINAL TEST SET\n","# =============================================================================\n","\n","print(\"\\n11. Evaluating model on validation and test sets...\")\n","\n","# Evaluate on validation set (used during training)\n","val_results = trainer.evaluate(eval_dataset=tokenized_datasets[\"validation\"])\n","print(\"Validation Results:\")\n","for key, value in val_results.items():\n","    if key.startswith('eval_'):\n","        print(f\"  {key}: {value:.4f}\")\n","\n","# Final evaluation on hold-out test set\n","test_results = trainer.evaluate(eval_dataset=tokenized_datasets[\"test\"])\n","print(\"\\nFinal Test Results:\")\n","for key, value in test_results.items():\n","    if key.startswith('eval_'):\n","        print(f\"  {key}: {value:.4f}\")\n","\n","# Detailed predictions on test set\n","test_predictions = trainer.predict(tokenized_datasets[\"test\"])\n","y_pred = np.argmax(test_predictions.predictions, axis=1)\n","y_true = test_predictions.label_ids\n","\n","# Classification report\n","report = classification_report(\n","    y_true, y_pred,\n","    target_names=[ENGLISH_NAMES[i] for i in range(10)],\n","    output_dict=True,\n","    zero_division=0\n",")\n","\n","print(\"\\nDetailed Test Set Classification Report:\")\n","print(classification_report(\n","    y_true, y_pred,\n","    target_names=[ENGLISH_NAMES[i] for i in range(10)],\n","    zero_division=0\n","))\n","\n","# Confusion matrix\n","cm = confusion_matrix(y_true, y_pred)\n","\n","# =============================================================================\n","# 12. CROSS-VALIDATION FOR ROBUST EVALUATION (OPTIONAL)\n","# =============================================================================\n","\n","if USE_CROSS_VALIDATION and len(X_train) < 1000:\n","    print(\"\\n12. Performing cross-validation for robust evaluation...\")\n","\n","    from sklearn.model_selection import StratifiedKFold\n","\n","    # Use original training + validation data for CV\n","    X_cv = np.concatenate([X_train, X_val])\n","    y_cv = np.concatenate([y_train, y_val])\n","\n","    skf = StratifiedKFold(n_splits=CV_FOLDS, shuffle=True, random_state=42)\n","    cv_scores = []\n","\n","    for fold, (train_idx, val_idx) in enumerate(skf.split(X_cv, y_cv)):\n","        print(f\"Training fold {fold+1}/{CV_FOLDS}...\")\n","\n","        # Create fold datasets\n","        X_fold_train, X_fold_val = X_cv[train_idx], X_cv[val_idx]\n","        y_fold_train, y_fold_val = y_cv[train_idx], y_cv[val_idx]\n","\n","        # Create datasets for this fold\n","        fold_train_dataset = Dataset.from_dict({\n","            'text': X_fold_train,\n","            'labels': y_fold_train\n","        })\n","\n","        fold_val_dataset = Dataset.from_dict({\n","            'text': X_fold_val,\n","            'labels': y_fold_val\n","        })\n","\n","        # Tokenize\n","        fold_train_tokenized = fold_train_dataset.map(tokenize_function, batched=True, remove_columns=['text'])\n","        fold_val_tokenized = fold_val_dataset.map(tokenize_function, batched=True, remove_columns=['text'])\n","\n","        # Create new model for this fold (reset weights)\n","        fold_model = AutoModelForSequenceClassification.from_pretrained(\n","            MODEL_ID,\n","            num_labels=10,\n","            trust_remote_code=True\n","        )\n","        fold_model = get_peft_model(fold_model, peft_config)\n","\n","        # Create trainer for this fold\n","        fold_trainer = AdvancedRegularizedTrainer(\n","            model=fold_model,\n","            args=TrainingArguments(\n","                output_dir=os.path.join(OUTPUT_DIR, f\"cv_fold_{fold}\"),\n","                num_train_epochs=NUM_EPOCHS,\n","                per_device_train_batch_size=BATCH_SIZE,\n","                per_device_eval_batch_size=BATCH_SIZE,\n","                learning_rate=LEARNING_RATE,\n","                weight_decay=WEIGHT_DECAY,\n","                evaluation_strategy=\"no\",  # No evaluation during CV training\n","                save_strategy=\"no\",        # Don't save CV models\n","                logging_steps=1000,        # Reduce logging\n","                report_to=\"none\",\n","                bf16=torch.cuda.is_available(),\n","            ),\n","            train_dataset=fold_train_tokenized,\n","            eval_dataset=fold_val_tokenized,\n","            compute_metrics=compute_metrics,\n","        )\n","\n","        # Train fold\n","        fold_trainer.train()\n","\n","        # Evaluate fold\n","        fold_results = fold_trainer.evaluate()\n","        cv_scores.append(fold_results['eval_f1_macro'])\n","\n","        print(f\"Fold {fold+1} F1-Macro: {fold_results['eval_f1_macro']:.4f}\")\n","\n","        # Clean up memory\n","        del fold_model, fold_trainer\n","        torch.cuda.empty_cache() if torch.cuda.is_available() else None\n","\n","    print(f\"\\nCross-validation results:\")\n","    print(f\"Mean F1-Macro: {np.mean(cv_scores):.4f} ± {np.std(cv_scores):.4f}\")\n","    print(f\"Individual folds: {cv_scores}\")\n","\n","    # Add CV results to final results\n","    cv_results = {\n","        \"cv_f1_macro_mean\": np.mean(cv_scores),\n","        \"cv_f1_macro_std\": np.std(cv_scores),\n","        \"cv_scores\": cv_scores\n","    }\n","else:\n","    print(\"Skipping cross-validation\")\n","    cv_results = {}\n","\n","# =============================================================================\n","# 12. VISUALIZATION AND SAVING RESULTS\n","# =============================================================================\n","\n","print(\"\\n12. Saving results and visualizations...\")\n","\n","# Save comprehensive results\n","results = {\n","    \"training_args\": training_args.to_dict(),\n","    \"model_config\": {\n","        \"base_model\": MODEL_ID,\n","        \"max_length\": MAX_LENGTH,\n","        \"lora_config\": {\n","            \"r\": LORA_R,\n","            \"alpha\": LORA_ALPHA,\n","            \"dropout\": LORA_DROPOUT\n","        },\n","        \"regularization\": {\n","            \"weight_decay\": WEIGHT_DECAY,\n","            \"label_smoothing\": 0.1,\n","            \"gradient_clipping\": 1.0,\n","            \"layer_freezing\": len(X_train) < 1000\n","        }\n","    },\n","    \"dataset_info\": {\n","        \"total_samples\": len(X),\n","        \"train_samples\": len(X_train),\n","        \"val_samples\": len(X_val),\n","        \"test_samples\": len(X_test),\n","        \"augmentation_applied\": USE_AUGMENTATION,\n","        \"augmentation_ratio\": AUG_RATIO if USE_AUGMENTATION else 0\n","    },\n","    \"training_results\": {\n","        \"final_loss\": training_result.training_loss,\n","        \"train_steps\": training_result.global_step\n","    },\n","    \"validation_results\": val_results,\n","    \"test_results\": test_results,\n","    \"classification_report\": report,\n","    \"confusion_matrix\": cm.tolist(),\n","    \"class_distribution\": {\n","        \"train\": train_dist.to_dict(),\n","        \"validation\": val_dist.to_dict(),\n","        \"test\": test_dist.to_dict()\n","    },\n","    \"class_weights\": class_weight_dict,\n","    \"cross_validation\": cv_results,\n","    \"regularization_techniques\": [\n","        \"LoRA with reduced rank\",\n","        \"Higher dropout rate\",\n","        \"Strong weight decay\",\n","        \"Label smoothing\",\n","        \"Gradient clipping\",\n","        \"Early stopping\",\n","        \"Layer freezing for small datasets\",\n","        \"Data augmentation\",\n","        \"Class balancing\",\n","        \"Cosine learning rate schedule\"\n","    ],\n","    \"timestamp\": datetime.now().isoformat()\n","}\n","\n","# Save results\n","results_file = os.path.join(OUTPUT_DIR, \"training_results.json\")\n","with open(results_file, 'w', encoding='utf-8') as f:\n","    json.dump(results, f, ensure_ascii=False, indent=2)\n","\n","# Create confusion matrix plot\n","plt.figure(figsize=(12, 10))\n","sns.heatmap(\n","    cm,\n","    annot=True,\n","    fmt='d',\n","    cmap='Blues',\n","    xticklabels=[ENGLISH_NAMES[i] for i in range(10)],\n","    yticklabels=[ENGLISH_NAMES[i] for i in range(10)]\n",")\n","plt.title('Confusion Matrix - NACC Complaint Classification')\n","plt.xlabel('Predicted')\n","plt.ylabel('Actual')\n","plt.xticks(rotation=45, ha='right')\n","plt.yticks(rotation=0)\n","plt.tight_layout()\n","plt.savefig(os.path.join(OUTPUT_DIR, \"confusion_matrix.png\"), dpi=300, bbox_inches='tight')\n","plt.close()\n","\n","# Create class distribution plot\n","fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n","\n","# Train distribution\n","train_dist.plot(kind='bar', ax=ax1, color='skyblue')\n","ax1.set_title('Training Set Class Distribution')\n","ax1.set_xlabel('Category ID')\n","ax1.set_ylabel('Count')\n","ax1.tick_params(axis='x', rotation=0)\n","\n","# Test distribution\n","test_dist.plot(kind='bar', ax=ax2, color='lightcoral')\n","ax2.set_title('Test Set Class Distribution')\n","ax2.set_xlabel('Category ID')\n","ax2.set_ylabel('Count')\n","ax2.tick_params(axis='x', rotation=0)\n","\n","plt.tight_layout()\n","plt.savefig(os.path.join(OUTPUT_DIR, \"class_distribution.png\"), dpi=300, bbox_inches='tight')\n","plt.close()\n","\n","print(f\"Visualizations saved to: {OUTPUT_DIR}\")\n","\n","# =============================================================================\n","# 13. SAVE MODEL\n","# =============================================================================\n","\n","print(\"\\n13. Saving trained model...\")\n","\n","# Save model and tokenizer\n","model_save_path = os.path.join(OUTPUT_DIR, \"model\")\n","tokenizer_save_path = os.path.join(OUTPUT_DIR, \"tokenizer\")\n","\n","trainer.save_model(model_save_path)\n","tokenizer.save_pretrained(tokenizer_save_path)\n","\n","print(f\"Model saved to: {model_save_path}\")\n","print(f\"Tokenizer saved to: {tokenizer_save_path}\")\n","\n","# =============================================================================\n","# 14. SAVE TEST SET\n","# =============================================================================\n","\n","print(\"\\n14. Saving test set for future evaluation...\")\n","\n","# Create test set with predictions\n","test_df = pd.DataFrame({\n","    'complaint': X_test,\n","    'true_category': y_true,\n","    'predicted_category': y_pred,\n","    'true_category_thai': [NUMERIC_TO_CATEGORY[cat] for cat in y_true],\n","    'predicted_category_thai': [NUMERIC_TO_CATEGORY[cat] for cat in y_pred],\n","    'true_category_english': [ENGLISH_NAMES[cat] for cat in y_true],\n","    'predicted_category_english': [ENGLISH_NAMES[cat] for cat in y_pred],\n","    'correct_prediction': y_true == y_pred\n","})\n","\n","test_csv_path = os.path.join(OUTPUT_DIR, \"test_set_with_predictions.csv\")\n","test_df.to_csv(test_csv_path, index=False, encoding='utf-8')\n","\n","print(f\"Test set with predictions saved to: {test_csv_path}\")\n","\n","# =============================================================================\n","# 15. FINAL SUMMARY\n","# =============================================================================\n","\n","print(\"\\n\" + \"=\" * 80)\n","print(\"TRAINING COMPLETED SUCCESSFULLY!\")\n","print(\"=\" * 80)\n","\n","print(f\"\\nKey Results:\")\n","print(f\"  Accuracy: {eval_results['eval_accuracy']:.3f}\")\n","print(f\"  Macro F1: {eval_results['eval_f1_macro']:.3f}\")\n","print(f\"  Weighted F1: {eval_results['eval_f1_weighted']:.3f}\")\n","\n","print(f\"\\nFiles saved to: {OUTPUT_DIR}\")\n","print(f\"  - Model: {model_save_path}\")\n","print(f\"  - Tokenizer: {tokenizer_save_path}\")\n","print(f\"  - Results: {results_file}\")\n","print(f\"  - Test set: {test_csv_path}\")\n","print(f\"  - Confusion matrix: confusion_matrix.png\")\n","print(f\"  - Class distribution: class_distribution.png\")\n","\n","print(f\"\\nModel ready for deployment and evaluation!\")"]},{"cell_type":"code","source":["# Improved Fine-tuning Code for NACC Complaint Classification\n","\n","# Install required packages\n","import subprocess\n","import sys\n","\n","def install_package(package):\n","    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package])\n","\n","# Install required packages\n","print(\"Installing required packages...\")\n","try:\n","    install_package(\"bitsandbytes\")\n","    install_package(\"peft\")\n","    install_package(\"accelerate\")\n","    install_package(\"transformers>=4.35.0\")\n","    install_package(\"datasets\")\n","    install_package(\"scikit-learn\")\n","    install_package(\"matplotlib\")\n","    install_package(\"seaborn\")\n","    print(\"All packages installed successfully!\")\n","except Exception as e:\n","    print(f\"Error installing packages: {e}\")\n","\n","import os\n","import json\n","import pandas as pd\n","import numpy as np\n","import torch\n","from datetime import datetime\n","from sklearn.model_selection import train_test_split, StratifiedKFold\n","from sklearn.utils.class_weight import compute_class_weight\n","from sklearn.metrics import accuracy_score, f1_score, precision_recall_fscore_support, classification_report, confusion_matrix\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","\n","from datasets import Dataset, DatasetDict\n","from transformers import (\n","    AutoTokenizer,\n","    AutoModelForSequenceClassification,\n","    BitsAndBytesConfig,\n","    TrainingArguments,\n","    Trainer,\n","    EarlyStoppingCallback\n",")\n","from peft import get_peft_model, LoraConfig, TaskType\n","import warnings\n","warnings.filterwarnings('ignore')\n","\n","# =============================================================================\n","# CONFIGURATION\n","# =============================================================================\n","\n","# Paths\n","DATASET_PATH = \"/content/drive/MyDrive/Government/KPI/NACC AI Project/[00] NACC LLM/Paper Revision/data/Trainset.csv\"\n","OUTPUT_DIR = \"/content/drive/MyDrive/Government/KPI/NACC AI Project/[00] NACC LLM/Paper Revision/finetune_new\"\n","\n","# Model configuration\n","MODEL_ID = \"scb10x/llama3.2-typhoon2-3b-instruct\"\n","MAX_LENGTH = 512  # Shorter sequences for efficiency\n","BATCH_SIZE = 8    # Larger batch size\n","GRADIENT_ACCUMULATION = 2\n","NUM_EPOCHS = 5    # More epochs for gradual learning\n","LEARNING_RATE = 1e-5  # Even lower learning rate\n","LORA_R = 8        # Smaller rank to reduce overfitting\n","LORA_ALPHA = 16   # Adjusted alpha\n","LORA_DROPOUT = 0.2  # Higher dropout\n","WEIGHT_DECAY = 0.1  # Strong regularization\n","\n","# Data augmentation parameters\n","AUG_RATIO = 0.3   # 30% of data will be augmented\n","USE_AUGMENTATION = True\n","USE_CROSS_VALIDATION = True\n","CV_FOLDS = 5\n","\n","# Create output directory\n","os.makedirs(OUTPUT_DIR, exist_ok=True)\n","\n","print(\"=\" * 80)\n","print(\"IMPROVED NACC COMPLAINT CLASSIFICATION FINE-TUNING\")\n","print(\"=\" * 80)\n","\n","# =============================================================================\n","# 1. CATEGORY MAPPING\n","# =============================================================================\n","\n","print(\"\\n1. Setting up category mapping...\")\n","\n","CATEGORY_MAPPING = {\n","    \"ปฏิบัติหรือละเว้นการปฏิบัติหน้าที่โดยมิชอบ\": 0,\n","    \"ทุจริตในการจัดทำงบประมาณ/โครงการ/เบิกจ่ายเงินในโครงการเป็นเท็จ\": 1,\n","    \"จัดซื้อจัดจ้าง\": 2,\n","    \"ออกเอกสารสิทธิที่ดิน\": 3,\n","    \"ยักยอก/เบียดบังเงินหรือทรัพย์สินของราชการ\": 4,\n","    \"การบริหารงานบุคคล (การบรรจุ/แต่งตั้ง/เลื่อนตำแหน่ง/โยกย้าย/ลงโทษวินัย)\": 5,\n","    \"ร่ำรวยผิดปกติ\": 6,\n","    \"เรียกรับสินบน\": 7,\n","    \"การขัดกันระหว่างประโยชน์ส่วนบุคคลกับประโยชน์ส่วนรวม\": 8,\n","    \"ก่าเกื้นจริยธรรม\": 9\n","}\n","\n","NUMERIC_TO_CATEGORY = {v: k for k, v in CATEGORY_MAPPING.items()}\n","\n","ENGLISH_NAMES = {\n","    0: \"Abuse of Power\",\n","    1: \"Budget/Project Fraud\",\n","    2: \"Procurement Fraud\",\n","    3: \"Fraudulent Land Title\",\n","    4: \"Embezzlement\",\n","    5: \"Personnel Misconduct\",\n","    6: \"Unusual Wealth\",\n","    7: \"Bribery\",\n","    8: \"Conflict of Interest\",\n","    9: \"Ethical Misconduct\"\n","}\n","\n","# Save mapping\n","mapping_file = os.path.join(OUTPUT_DIR, \"category_mapping.json\")\n","with open(mapping_file, 'w', encoding='utf-8') as f:\n","    json.dump({\n","        \"thai_to_numeric\": CATEGORY_MAPPING,\n","        \"numeric_to_thai\": NUMERIC_TO_CATEGORY,\n","        \"english_names\": ENGLISH_NAMES\n","    }, f, ensure_ascii=False, indent=2)\n","\n","print(f\"Category mapping saved to: {mapping_file}\")\n","\n","# =============================================================================\n","# 2. DATA LOADING AND PREPROCESSING\n","# =============================================================================\n","\n","print(\"\\n2. Loading and preprocessing data...\")\n","\n","# Load data\n","try:\n","    df = pd.read_csv(DATASET_PATH, encoding='utf-8')\n","    print(f\"Loaded dataset with {len(df)} samples\")\n","    print(f\"Columns: {list(df.columns)}\")\n","except Exception as e:\n","    print(f\"Error loading dataset: {e}\")\n","    raise\n","\n","# Check data structure\n","print(\"\\nData structure:\")\n","print(df.head())\n","print(f\"\\nDataset shape: {df.shape}\")\n","\n","# Map categories to numeric\n","df['category_numeric'] = df['category'].map(CATEGORY_MAPPING)\n","\n","# Check for unmapped categories\n","unmapped = df[df['category_numeric'].isna()]\n","if len(unmapped) > 0:\n","    print(f\"Warning: {len(unmapped)} unmapped categories found:\")\n","    print(unmapped['category'].unique())\n","    df = df.dropna(subset=['category_numeric'])\n","\n","# Convert to int\n","df['category_numeric'] = df['category_numeric'].astype(int)\n","\n","print(f\"Final dataset size: {len(df)}\")\n","\n","# Extract features and labels\n","X = df['complaint'].values\n","y = df['category_numeric'].values\n","\n","# Check class distribution\n","class_dist = df['category_numeric'].value_counts().sort_index()\n","print(\"\\nClass distribution:\")\n","for cat_id, count in class_dist.items():\n","    english_name = ENGLISH_NAMES[cat_id]\n","    percentage = (count / len(df)) * 100\n","    print(f\"{cat_id}: {english_name} - {count} samples ({percentage:.1f}%)\")\n","\n","# =============================================================================\n","# 3. DATA AUGMENTATION FOR SMALL DATASET\n","# =============================================================================\n","\n","print(\"\\n3. Implementing data augmentation...\")\n","\n","def simple_back_translation_augment(text):\n","    \"\"\"Simple augmentation through word shuffling and synonym replacement\"\"\"\n","    import random\n","\n","    # Simple word shuffling (keeping sentence structure)\n","    sentences = text.split('.')\n","    augmented_sentences = []\n","\n","    for sentence in sentences:\n","        words = sentence.strip().split()\n","        if len(words) > 3:\n","            # Shuffle middle words occasionally\n","            if random.random() < 0.3:\n","                middle = words[1:-1]\n","                random.shuffle(middle)\n","                words = [words[0]] + middle + [words[-1]]\n","        augmented_sentences.append(' '.join(words))\n","\n","    return '.'.join(augmented_sentences)\n","\n","def character_level_augment(text):\n","    \"\"\"Character-level augmentation for Thai text\"\"\"\n","    import random\n","\n","    chars = list(text)\n","    if len(chars) > 10:\n","        # Random character swapping (very conservative)\n","        if random.random() < 0.1:\n","            idx1, idx2 = random.sample(range(1, len(chars)-1), 2)\n","            chars[idx1], chars[idx2] = chars[idx2], chars[idx1]\n","\n","    return ''.join(chars)\n","\n","def augment_text(text, method='shuffle'):\n","    \"\"\"Apply text augmentation\"\"\"\n","    if method == 'shuffle':\n","        return simple_back_translation_augment(text)\n","    elif method == 'char':\n","        return character_level_augment(text)\n","    else:\n","        return text\n","\n","def create_augmented_data(X, y, augmentation_ratio=0.3):\n","    \"\"\"Create augmented dataset\"\"\"\n","    print(f\"Creating augmented data with ratio: {augmentation_ratio}\")\n","\n","    # Calculate samples to augment per class\n","    unique_classes, class_counts = np.unique(y, return_counts=True)\n","\n","    X_aug = []\n","    y_aug = []\n","\n","    for class_id in unique_classes:\n","        class_indices = np.where(y == class_id)[0]\n","        class_texts = X[class_indices]\n","\n","        # Calculate how many samples to augment for this class\n","        current_count = len(class_indices)\n","        target_augment = int(current_count * augmentation_ratio)\n","\n","        # Randomly select samples to augment\n","        aug_indices = np.random.choice(len(class_texts), size=target_augment, replace=True)\n","\n","        for idx in aug_indices:\n","            original_text = class_texts[idx]\n","\n","            # Apply different augmentation methods\n","            aug_method = np.random.choice(['shuffle', 'char'], p=[0.7, 0.3])\n","            augmented_text = augment_text(original_text, method=aug_method)\n","\n","            # Only add if augmentation actually changed the text\n","            if augmented_text != original_text:\n","                X_aug.append(augmented_text)\n","                y_aug.append(class_id)\n","\n","    print(f\"Generated {len(X_aug)} augmented samples\")\n","    return np.array(X_aug), np.array(y_aug)\n","\n","# Apply data augmentation if enabled\n","if USE_AUGMENTATION and len(df) < 1000:  # Only for small datasets\n","    print(\"Dataset is small, applying data augmentation...\")\n","    X_aug, y_aug = create_augmented_data(X, y, AUG_RATIO)\n","\n","    # Combine original and augmented data\n","    X_combined = np.concatenate([X, X_aug])\n","    y_combined = np.concatenate([y, y_aug])\n","\n","    print(f\"Original dataset: {len(X)} samples\")\n","    print(f\"Augmented dataset: {len(X_combined)} samples\")\n","\n","    # Use combined data for training\n","    X, y = X_combined, y_combined\n","else:\n","    print(\"Skipping augmentation (dataset size sufficient or disabled)\")\n","\n","# =============================================================================\n","# 4. STRATIFIED TRAIN-TEST SPLIT WITH CROSS-VALIDATION\n","# =============================================================================\n","\n","print(\"\\n4. Creating stratified splits...\")\n","\n","# First, create a hold-out test set (20%)\n","X_temp, X_test, y_temp, y_test = train_test_split(\n","    X, y,\n","    test_size=0.2,\n","    stratify=y,\n","    random_state=42\n",")\n","\n","# Then split remaining data for train/validation\n","X_train, X_val, y_train, y_val = train_test_split(\n","    X_temp, y_temp,\n","    test_size=0.2,  # 20% of remaining 80% = 16% of total\n","    stratify=y_temp,\n","    random_state=42\n",")\n","\n","print(f\"Train set: {len(X_train)} samples ({len(X_train)/len(X)*100:.1f}%)\")\n","print(f\"Validation set: {len(X_val)} samples ({len(X_val)/len(X)*100:.1f}%)\")\n","print(f\"Test set: {len(X_test)} samples ({len(X_test)/len(X)*100:.1f}%)\")\n","\n","# Check distributions\n","train_dist = pd.Series(y_train).value_counts().sort_index()\n","val_dist = pd.Series(y_val).value_counts().sort_index()\n","test_dist = pd.Series(y_test).value_counts().sort_index()\n","\n","print(\"\\nClass distributions:\")\n","for cat_id in range(10):\n","    train_count = train_dist.get(cat_id, 0)\n","    val_count = val_dist.get(cat_id, 0)\n","    test_count = test_dist.get(cat_id, 0)\n","    print(f\"  Class {cat_id}: Train={train_count}, Val={val_count}, Test={test_count}\")\n","\n","# =============================================================================\n","# 5. COMPUTE CLASS WEIGHTS FOR IMBALANCED DATA\n","# =============================================================================\n","\n","print(\"\\n5. Computing class weights...\")\n","\n","class_weights = compute_class_weight(\n","    'balanced',\n","    classes=np.unique(y_train),\n","    y=y_train\n",")\n","\n","class_weight_dict = {i: weight for i, weight in enumerate(class_weights)}\n","print(\"Class weights:\")\n","for cat_id, weight in class_weight_dict.items():\n","    if cat_id < len(ENGLISH_NAMES):\n","        print(f\"  {cat_id} ({ENGLISH_NAMES[cat_id]}): {weight:.3f}\")\n","\n","# =============================================================================\n","# 6. CREATE DATASETS WITH VALIDATION\n","# =============================================================================\n","\n","print(\"\\n6. Creating datasets...\")\n","\n","# Create datasets\n","train_dataset = Dataset.from_dict({\n","    'text': X_train,\n","    'labels': y_train\n","})\n","\n","val_dataset = Dataset.from_dict({\n","    'text': X_val,\n","    'labels': y_val\n","})\n","\n","test_dataset = Dataset.from_dict({\n","    'text': X_test,\n","    'labels': y_test\n","})\n","\n","# Create DatasetDict\n","datasets = DatasetDict({\n","    'train': train_dataset,\n","    'validation': val_dataset,\n","    'test': test_dataset\n","})\n","\n","print(\"Datasets created successfully\")\n","\n","# =============================================================================\n","# 7. MODEL AND TOKENIZER SETUP\n","# =============================================================================\n","\n","print(\"\\n7. Setting up model and tokenizer...\")\n","\n","# Load tokenizer\n","tokenizer = AutoTokenizer.from_pretrained(MODEL_ID)\n","if tokenizer.pad_token is None:\n","    tokenizer.pad_token = tokenizer.eos_token\n","\n","print(\"Tokenizer loaded\")\n","\n","# Setup quantization config with fallback\n","try:\n","    quantization_config = BitsAndBytesConfig(\n","        load_in_4bit=True,\n","        bnb_4bit_use_double_quant=True,\n","        bnb_4bit_compute_dtype=torch.bfloat16,\n","        bnb_4bit_quant_type=\"nf4\"\n","    )\n","    use_quantization = True\n","    print(\"4-bit quantization enabled\")\n","except Exception as e:\n","    print(f\"Quantization not available: {e}\")\n","    print(\"Loading model without quantization...\")\n","    quantization_config = None\n","    use_quantization = False\n","\n","# Load model for classification\n","model_kwargs = {\n","    \"num_labels\": 10,  # Number of categories\n","    \"device_map\": \"auto\",\n","    \"trust_remote_code\": True\n","}\n","\n","if use_quantization and quantization_config is not None:\n","    model_kwargs[\"quantization_config\"] = quantization_config\n","\n","try:\n","    model = AutoModelForSequenceClassification.from_pretrained(\n","        MODEL_ID,\n","        **model_kwargs\n","    )\n","except Exception as e:\n","    print(f\"Error loading model with quantization: {e}\")\n","    print(\"Retrying without quantization...\")\n","    # Fallback without quantization\n","    model = AutoModelForSequenceClassification.from_pretrained(\n","        MODEL_ID,\n","        num_labels=10,\n","        trust_remote_code=True\n","    )\n","\n","print(\"Base model loaded\")\n","\n","# Apply LoRA with stronger regularization for small datasets\n","peft_config = LoraConfig(\n","    r=LORA_R,           # Smaller rank reduces parameters\n","    lora_alpha=LORA_ALPHA,\n","    lora_dropout=LORA_DROPOUT,  # Higher dropout\n","    bias=\"none\",\n","    task_type=TaskType.SEQ_CLS,\n","    target_modules=[\"q_proj\", \"v_proj\", \"k_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"]\n",")\n","\n","model = get_peft_model(model, peft_config)\n","\n","# Freeze more layers for small dataset\n","if len(X_train) < 1000:\n","    print(\"Small dataset detected. Applying additional regularization...\")\n","\n","    # Freeze embedding layers\n","    for name, param in model.named_parameters():\n","        if 'embed' in name.lower():\n","            param.requires_grad = False\n","\n","    # Freeze some transformer layers (freeze bottom layers, train top layers)\n","    total_layers = len([n for n, p in model.named_parameters() if 'layers.' in n and 'weight' in n])\n","    layers_to_freeze = total_layers // 3  # Freeze bottom 1/3 of layers\n","\n","    for name, param in model.named_parameters():\n","        if 'layers.' in name:\n","            layer_num = int(name.split('layers.')[1].split('.')[0])\n","            if layer_num < layers_to_freeze:\n","                param.requires_grad = False\n","\n","model.print_trainable_parameters()\n","\n","# =============================================================================\n","# 8. TOKENIZATION\n","# =============================================================================\n","\n","print(\"\\n8. Tokenizing data...\")\n","\n","def tokenize_function(examples):\n","    \"\"\"Tokenize complaints for classification\"\"\"\n","    return tokenizer(\n","        examples['text'],\n","        truncation=True,\n","        padding=True,\n","        max_length=MAX_LENGTH,\n","        return_tensors=None\n","    )\n","\n","# Tokenize datasets\n","tokenized_datasets = datasets.map(\n","    tokenize_function,\n","    batched=True,\n","    remove_columns=['text']\n",")\n","\n","print(\"Tokenization completed\")\n","\n","# =============================================================================\n","# 9. EVALUATION METRICS\n","# =============================================================================\n","\n","def compute_metrics(eval_pred):\n","    \"\"\"Compute comprehensive evaluation metrics\"\"\"\n","    predictions, labels = eval_pred\n","    predictions = np.argmax(predictions, axis=1)\n","\n","    # Basic metrics\n","    accuracy = accuracy_score(labels, predictions)\n","\n","    # Per-class metrics\n","    precision, recall, f1, support = precision_recall_fscore_support(\n","        labels, predictions, average=None, zero_division=0\n","    )\n","\n","    # Macro and weighted averages\n","    f1_macro = f1_score(labels, predictions, average='macro', zero_division=0)\n","    f1_weighted = f1_score(labels, predictions, average='weighted', zero_division=0)\n","\n","    return {\n","        'accuracy': accuracy,\n","        'f1_macro': f1_macro,\n","        'f1_weighted': f1_weighted,\n","        'precision_macro': np.mean(precision),\n","        'recall_macro': np.mean(recall)\n","    }\n","\n","# =============================================================================\n","# 10. TRAINING SETUP\n","# =============================================================================\n","\n","print(\"\\n10. Setting up training...\")\n","\n","# Custom trainer with advanced regularization for small datasets\n","class AdvancedRegularizedTrainer(Trainer):\n","    def __init__(self, class_weights=None, mixup_alpha=0.2, label_smoothing=0.1, **kwargs):\n","        super().__init__(**kwargs)\n","        self.class_weights = class_weights\n","        self.mixup_alpha = mixup_alpha\n","        self.label_smoothing = label_smoothing\n","\n","    def compute_loss(self, model, inputs, return_outputs=False):\n","        labels = inputs.get(\"labels\")\n","        outputs = model(**inputs)\n","        logits = outputs.get('logits')\n","\n","        # Apply class weights\n","        if self.class_weights is not None:\n","            weight_tensor = torch.tensor(list(self.class_weights.values()),\n","                                       dtype=torch.float32, device=labels.device)\n","        else:\n","            weight_tensor = None\n","\n","        # Label smoothing for regularization\n","        if self.label_smoothing > 0:\n","            loss_fct = torch.nn.CrossEntropyLoss(\n","                weight=weight_tensor,\n","                label_smoothing=self.label_smoothing\n","            )\n","        else:\n","            loss_fct = torch.nn.CrossEntropyLoss(weight=weight_tensor)\n","\n","        loss = loss_fct(logits, labels)\n","\n","        # Add L2 regularization to LoRA parameters\n","        l2_reg = 0\n","        for name, param in model.named_parameters():\n","            if 'lora_' in name and param.requires_grad:\n","                l2_reg += torch.norm(param, p=2)\n","\n","        loss = loss + 0.01 * l2_reg  # L2 regularization coefficient\n","\n","        return (loss, outputs) if return_outputs else loss\n","\n","# Enhanced training arguments for small datasets\n","# Using eval_strategy instead of evaluation_strategy for compatibility\n","training_args = TrainingArguments(\n","    output_dir=os.path.join(OUTPUT_DIR, \"checkpoints\"),\n","    num_train_epochs=NUM_EPOCHS,\n","    per_device_train_batch_size=BATCH_SIZE,\n","    per_device_eval_batch_size=BATCH_SIZE,\n","    gradient_accumulation_steps=GRADIENT_ACCUMULATION,\n","    learning_rate=LEARNING_RATE,\n","    weight_decay=WEIGHT_DECAY,\n","    warmup_ratio=0.1,\n","    lr_scheduler_type=\"cosine\",\n","    logging_steps=10,\n","    eval_strategy=\"steps\",  # Changed from evaluation_strategy\n","    eval_steps=50,\n","    save_strategy=\"steps\",\n","    save_steps=50,\n","    save_total_limit=5,\n","    load_best_model_at_end=True,\n","    metric_for_best_model=\"eval_f1_macro\",\n","    greater_is_better=True,\n","    report_to=\"none\",\n","    dataloader_pin_memory=False,\n","    bf16=torch.cuda.is_available(),\n","    fp16=not torch.cuda.is_available(),\n","    optim=\"adamw_torch\",\n","    max_grad_norm=1.0,\n","    dataloader_drop_last=True,\n","    run_name=f\"nacc-classification-{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",")\n","\n","# Create trainer with advanced regularization\n","trainer = AdvancedRegularizedTrainer(\n","    model=model,\n","    args=training_args,\n","    train_dataset=tokenized_datasets[\"train\"],\n","    eval_dataset=tokenized_datasets[\"validation\"],  # Use validation set\n","    compute_metrics=compute_metrics,\n","    class_weights=class_weight_dict,\n","    mixup_alpha=0.2 if len(X_train) < 1000 else 0.0,  # Enable mixup for small datasets\n","    label_smoothing=0.1,  # Label smoothing for regularization\n","    callbacks=[\n","        EarlyStoppingCallback(early_stopping_patience=5, early_stopping_threshold=0.001)\n","    ]\n",")\n","\n","print(\"Trainer created successfully\")\n","\n","# =============================================================================\n","# 11. TRAINING\n","# =============================================================================\n","\n","print(\"\\n11. Starting training...\")\n","print(\"=\" * 50)\n","\n","# Train the model\n","training_result = trainer.train()\n","\n","print(\"Training completed!\")\n","print(f\"Final training loss: {training_result.training_loss:.4f}\")\n","\n","# =============================================================================\n","# 12. EVALUATION WITH FINAL TEST SET\n","# =============================================================================\n","\n","print(\"\\n12. Evaluating model on validation and test sets...\")\n","\n","# Evaluate on validation set (used during training)\n","val_results = trainer.evaluate(eval_dataset=tokenized_datasets[\"validation\"])\n","print(\"Validation Results:\")\n","for key, value in val_results.items():\n","    if key.startswith('eval_'):\n","        print(f\"  {key}: {value:.4f}\")\n","\n","# Final evaluation on hold-out test set\n","test_results = trainer.evaluate(eval_dataset=tokenized_datasets[\"test\"])\n","print(\"\\nFinal Test Results:\")\n","for key, value in test_results.items():\n","    if key.startswith('eval_'):\n","        print(f\"  {key}: {value:.4f}\")\n","\n","# Detailed predictions on test set\n","test_predictions = trainer.predict(tokenized_datasets[\"test\"])\n","y_pred = np.argmax(test_predictions.predictions, axis=1)\n","y_true = test_predictions.label_ids\n","\n","# Classification report\n","report = classification_report(\n","    y_true, y_pred,\n","    target_names=[ENGLISH_NAMES[i] for i in range(10)],\n","    output_dict=True,\n","    zero_division=0\n",")\n","\n","print(\"\\nDetailed Test Set Classification Report:\")\n","print(classification_report(\n","    y_true, y_pred,\n","    target_names=[ENGLISH_NAMES[i] for i in range(10)],\n","    zero_division=0\n","))\n","\n","# Confusion matrix\n","cm = confusion_matrix(y_true, y_pred)\n","\n","# =============================================================================\n","# 13. CROSS-VALIDATION FOR ROBUST EVALUATION (OPTIONAL)\n","# =============================================================================\n","\n","if USE_CROSS_VALIDATION and len(X_train) < 1000:\n","    print(\"\\n13. Performing cross-validation for robust evaluation...\")\n","\n","    # Use original training + validation data for CV\n","    X_cv = np.concatenate([X_train, X_val])\n","    y_cv = np.concatenate([y_train, y_val])\n","\n","    skf = StratifiedKFold(n_splits=CV_FOLDS, shuffle=True, random_state=42)\n","    cv_scores = []\n","\n","    for fold, (train_idx, val_idx) in enumerate(skf.split(X_cv, y_cv)):\n","        print(f\"Training fold {fold+1}/{CV_FOLDS}...\")\n","\n","        # Create fold datasets\n","        X_fold_train, X_fold_val = X_cv[train_idx], X_cv[val_idx]\n","        y_fold_train, y_fold_val = y_cv[train_idx], y_cv[val_idx]\n","\n","        # Create datasets for this fold\n","        fold_train_dataset = Dataset.from_dict({\n","            'text': X_fold_train,\n","            'labels': y_fold_train\n","        })\n","\n","        fold_val_dataset = Dataset.from_dict({\n","            'text': X_fold_val,\n","            'labels': y_fold_val\n","        })\n","\n","        # Tokenize\n","        fold_train_tokenized = fold_train_dataset.map(tokenize_function, batched=True, remove_columns=['text'])\n","        fold_val_tokenized = fold_val_dataset.map(tokenize_function, batched=True, remove_columns=['text'])\n","\n","        # Create new model for this fold (reset weights)\n","        fold_model = AutoModelForSequenceClassification.from_pretrained(\n","            MODEL_ID,\n","            num_labels=10,\n","            trust_remote_code=True\n","        )\n","        fold_model = get_peft_model(fold_model, peft_config)\n","\n","        # Create trainer for this fold\n","        fold_training_args = TrainingArguments(\n","            output_dir=os.path.join(OUTPUT_DIR, f\"cv_fold_{fold}\"),\n","            num_train_epochs=NUM_EPOCHS,\n","            per_device_train_batch_size=BATCH_SIZE,\n","            per_device_eval_batch_size=BATCH_SIZE,\n","            learning_rate=LEARNING_RATE,\n","            weight_decay=WEIGHT_DECAY,\n","            eval_strategy=\"no\",  # Changed from evaluation_strategy\n","            save_strategy=\"no\",\n","            logging_steps=1000,\n","            report_to=\"none\",\n","            bf16=torch.cuda.is_available(),\n","        )\n","\n","        fold_trainer = AdvancedRegularizedTrainer(\n","            model=fold_model,\n","            args=fold_training_args,\n","            train_dataset=fold_train_tokenized,\n","            eval_dataset=fold_val_tokenized,\n","            compute_metrics=compute_metrics,\n","            class_weights=class_weight_dict\n","        )\n","\n","        # Train fold\n","        fold_trainer.train()\n","\n","        # Evaluate fold\n","        fold_results = fold_trainer.evaluate()\n","        cv_scores.append(fold_results['eval_f1_macro'])\n","\n","        print(f\"Fold {fold+1} F1-Macro: {fold_results['eval_f1_macro']:.4f}\")\n","\n","        # Clean up memory\n","        del fold_model, fold_trainer\n","        torch.cuda.empty_cache() if torch.cuda.is_available() else None\n","\n","    print(f\"\\nCross-validation results:\")\n","    print(f\"Mean F1-Macro: {np.mean(cv_scores):.4f} ± {np.std(cv_scores):.4f}\")\n","    print(f\"Individual folds: {cv_scores}\")\n","\n","    # Add CV results to final results\n","    cv_results = {\n","        \"cv_f1_macro_mean\": np.mean(cv_scores),\n","        \"cv_f1_macro_std\": np.std(cv_scores),\n","        \"cv_scores\": cv_scores\n","    }\n","else:\n","    print(\"\\n13. Skipping cross-validation\")\n","    cv_results = {}\n","\n","# =============================================================================\n","# 14. VISUALIZATION AND SAVING RESULTS\n","# =============================================================================\n","\n","print(\"\\n14. Saving results and visualizations...\")\n","\n","# Save comprehensive results\n","results = {\n","    \"training_args\": training_args.to_dict(),\n","    \"model_config\": {\n","        \"base_model\": MODEL_ID,\n","        \"max_length\": MAX_LENGTH,\n","        \"lora_config\": {\n","            \"r\": LORA_R,\n","            \"alpha\": LORA_ALPHA,\n","            \"dropout\": LORA_DROPOUT\n","        },\n","        \"regularization\": {\n","            \"weight_decay\": WEIGHT_DECAY,\n","            \"label_smoothing\": 0.1,\n","            \"gradient_clipping\": 1.0,\n","            \"layer_freezing\": len(X_train) < 1000\n","        }\n","    },\n","    \"dataset_info\": {\n","        \"total_samples\": len(X),\n","        \"train_samples\": len(X_train),\n","        \"val_samples\": len(X_val),\n","        \"test_samples\": len(X_test),\n","        \"augmentation_applied\": USE_AUGMENTATION,\n","        \"augmentation_ratio\": AUG_RATIO if USE_AUGMENTATION else 0\n","    },\n","    \"training_results\": {\n","        \"final_loss\": training_result.training_loss,\n","        \"train_steps\": training_result.global_step\n","    },\n","    \"validation_results\": val_results,\n","    \"test_results\": test_results,\n","    \"classification_report\": report,\n","    \"confusion_matrix\": cm.tolist(),\n","    \"class_distribution\": {\n","        \"train\": train_dist.to_dict(),\n","        \"validation\": val_dist.to_dict(),\n","        \"test\": test_dist.to_dict()\n","    },\n","    \"class_weights\": class_weight_dict,\n","    \"cross_validation\": cv_results,\n","    \"timestamp\": datetime.now().isoformat()\n","}\n","\n","# Save results\n","results_file = os.path.join(OUTPUT_DIR, \"training_results.json\")\n","with open(results_file, 'w', encoding='utf-8') as f:\n","    json.dump(results, f, ensure_ascii=False, indent=2)\n","\n","print(f\"Results saved to: {results_file}\")\n","\n","# Create confusion matrix plot\n","plt.figure(figsize=(12, 10))\n","sns.heatmap(\n","    cm,\n","    annot=True,\n","    fmt='d',\n","    cmap='Blues',\n","    xticklabels=[ENGLISH_NAMES[i] for i in range(10)],\n","    yticklabels=[ENGLISH_NAMES[i] for i in range(10)]\n",")\n","plt.title('Confusion Matrix - NACC Complaint Classification')\n","plt.xlabel('Predicted')\n","plt.ylabel('Actual')\n","plt.xticks(rotation=45, ha='right')\n","plt.yticks(rotation=0)\n","plt.tight_layout()\n","plt.savefig(os.path.join(OUTPUT_DIR, \"confusion_matrix.png\"), dpi=300, bbox_inches='tight')\n","plt.close()\n","\n","# Create class distribution plot\n","fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n","\n","# Train distribution\n","train_dist.plot(kind='bar', ax=ax1, color='skyblue')\n","ax1.set_title('Training Set Class Distribution')\n","ax1.set_xlabel('Category ID')\n","ax1.set_ylabel('Count')\n","ax1.tick_params(axis='x', rotation=0)\n","\n","# Test distribution\n","test_dist.plot(kind='bar', ax=ax2, color='lightcoral')\n","ax2.set_title('Test Set Class Distribution')\n","ax2.set_xlabel('Category ID')\n","ax2.set_ylabel('Count')\n","ax2.tick_params(axis='x', rotation=0)\n","\n","plt.tight_layout()\n","plt.savefig(os.path.join(OUTPUT_DIR, \"class_distribution.png\"), dpi=300, bbox_inches='tight')\n","plt.close()\n","\n","print(f\"Visualizations saved to: {OUTPUT_DIR}\")\n","\n","# =============================================================================\n","# 15. SAVE MODEL\n","# =============================================================================\n","\n","print(\"\\n15. Saving trained model...\")\n","\n","# Save model and tokenizer\n","model_save_path = os.path.join(OUTPUT_DIR, \"model\")\n","tokenizer_save_path = os.path.join(OUTPUT_DIR, \"tokenizer\")\n","\n","trainer.save_model(model_save_path)\n","tokenizer.save_pretrained(tokenizer_save_path)\n","\n","print(f\"Model saved to: {model_save_path}\")\n","print(f\"Tokenizer saved to: {tokenizer_save_path}\")\n","\n","# =============================================================================\n","# 16. SAVE TEST SET WITH PREDICTIONS\n","# =============================================================================\n","\n","print(\"\\n16. Saving test set with predictions for future evaluation...\")\n","\n","# Create test set with predictions\n","test_df = pd.DataFrame({\n","    'complaint': X_test,\n","    'true_category': y_true,\n","    'predicted_category': y_pred,\n","    'true_category_thai': [NUMERIC_TO_CATEGORY[cat] for cat in y_true],\n","    'predicted_category_thai': [NUMERIC_TO_CATEGORY[cat] for cat in y_pred],\n","    'true_category_english': [ENGLISH_NAMES[cat] for cat in y_true],\n","    'predicted_category_english': [ENGLISH_NAMES[cat] for cat in y_pred],\n","    'correct_prediction': y_true == y_pred\n","})\n","\n","test_csv_path = os.path.join(OUTPUT_DIR, \"test_set_with_predictions.csv\")\n","test_df.to_csv(test_csv_path, index=False, encoding='utf-8')\n","\n","print(f\"Test set with predictions saved to: {test_csv_path}\")\n","\n","# =============================================================================\n","# 17. FINAL SUMMARY\n","# =============================================================================\n","\n","print(\"\\n\" + \"=\" * 80)\n","print(\"TRAINING COMPLETED SUCCESSFULLY!\")\n","print(\"=\" * 80)\n","\n","print(f\"\\nKey Results:\")\n","print(f\"  Accuracy: {test_results['eval_accuracy']:.3f}\")\n","print(f\"  Macro F1: {test_results['eval_f1_macro']:.3f}\")\n","print(f\"  Weighted F1: {test_results['eval_f1_weighted']:.3f}\")\n","\n","print(f\"\\nFiles saved to: {OUTPUT_DIR}\")\n","print(f\"  - Model: {model_save_path}\")\n","print(f\"  - Tokenizer: {tokenizer_save_path}\")\n","print(f\"  - Results: {results_file}\")\n","print(f\"  - Test set: {test_csv_path}\")\n","print(f\"  - Confusion matrix: confusion_matrix.png\")\n","print(f\"  - Class distribution: class_distribution.png\")\n","\n","print(f\"\\nModel ready for deployment and evaluation!\")\n","\n","# =============================================================================\n","# 18. INFERENCE FUNCTION (FOR TESTING)\n","# =============================================================================\n","\n","def predict_complaint(text, model=model, tokenizer=tokenizer):\n","    \"\"\"\n","    Predict the category of a complaint text\n","\n","    Args:\n","        text: Input complaint text\n","        model: Trained model\n","        tokenizer: Tokenizer\n","\n","    Returns:\n","        Dictionary with prediction results\n","    \"\"\"\n","    # Tokenize input\n","    inputs = tokenizer(\n","        text,\n","        truncation=True,\n","        padding=True,\n","        max_length=MAX_LENGTH,\n","        return_tensors=\"pt\"\n","    )\n","\n","    # Move to device\n","    device = next(model.parameters()).device\n","    inputs = {k: v.to(device) for k, v in inputs.items()}\n","\n","    # Get predictions\n","    with torch.no_grad():\n","        outputs = model(**inputs)\n","        logits = outputs.logits\n","        probs = torch.softmax(logits, dim=-1)\n","\n","    # Get top prediction\n","    pred_idx = torch.argmax(probs, dim=-1).item()\n","    confidence = probs[0, pred_idx].item()\n","\n","    # Get top 3 predictions\n","    top3_probs, top3_indices = torch.topk(probs[0], k=min(3, len(probs[0])))\n","\n","    top3_predictions = []\n","    for prob, idx in zip(top3_probs, top3_indices):\n","        top3_predictions.append({\n","            \"category_id\": idx.item(),\n","            \"category_thai\": NUMERIC_TO_CATEGORY[idx.item()],\n","            \"category_english\": ENGLISH_NAMES[idx.item()],\n","            \"confidence\": prob.item()\n","        })\n","\n","    return {\n","        \"predicted_category_id\": pred_idx,\n","        \"predicted_category_thai\": NUMERIC_TO_CATEGORY[pred_idx],\n","        \"predicted_category_english\": ENGLISH_NAMES[pred_idx],\n","        \"confidence\": confidence,\n","        \"top3_predictions\": top3_predictions\n","    }\n","\n","# Test the inference function\n","print(\"\\n\" + \"=\" * 80)\n","print(\"TESTING INFERENCE FUNCTION\")\n","print(\"=\" * 80)\n","\n","# Example test\n","test_text = \"มีการเรียกรับเงินสินบนในการอนุมัติโครงการก่อสร้าง\"\n","print(f\"\\nTest text: {test_text}\")\n","\n","result = predict_complaint(test_text)\n","print(f\"\\nPrediction: {result['predicted_category_thai']}\")\n","print(f\"English: {result['predicted_category_english']}\")\n","print(f\"Confidence: {result['confidence']:.2%}\")\n","\n","print(\"\\nTop 3 predictions:\")\n","for i, pred in enumerate(result['top3_predictions'], 1):\n","    print(f\"  {i}. {pred['category_english']} ({pred['confidence']:.2%})\")\n","\n","print(\"\\n\" + \"=\" * 80)\n","print(\"ALL PROCESSES COMPLETED SUCCESSFULLY!\")\n","print(\"=\" * 80)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["84f6418b77c5474b8adeead02eedc065","e2f73c0533544e9a8eefcec8fb883cfa","1abecfb90c1243fa80f0ac2aaec579b8","39dc118625b3458e84e986e404ff8f0e","9aa5fa2a3e854af8abd1fae2d60a4fc7","89ed128c10944e9ba919def4a4fc142a","2f670b78b7434e6d9a0d5e5a2ff16bfa","99e01b114aca4c409be4e466d3297a26","8c3ae76df797411e900eadd70a02b7df","8dc7660d7ac34ee0a68d5f5446dfd067","d9ea530536a7455c94b2491e6963fa31","aada607319ba4409a67888bd28003837","34ef9f8c4ce448daa974263c7790f8f9","7a99c937e4e7408e97377f76c643d61e","08943ee3b41a41abb2d3c94f040c6ce9","86d1bab8a2834687b0bf876103089400","cceb4c0fada4433c9077d5a0da6da2e7","c6dd5583ccb3453ca72f30d0fae99671","ab0582f96072423ba62b9317ff55b0ef","57dcb3c479c848f3a7376d2b6c888dcd","af1a8dc1d8134e93ab1a56a19e2e62be","1bc6047512fd43f0ada8a8998192d183","030e45549b91424c83ccd5a9927e7dc3","1dff2a510f0c472d9c07dd968af04f40","64ab267c2b3c4984bad712af38b1fb02","e376aa34ff184cecb8188c55aa8383b6","892fd574afcd4276be80aff9caad6282","49b103ec920c457fa74b56eb92098dc5","a1fe7433534c4d9bb9aba36fe8c29472","fed3c0e49477408d98e6c11971877902","c4037bd9188d45eb93521e563a9136ae","20bee62e15364e208a133a34e549a181","74fa34c625bd4acebeb23c2cff3b4be7","4b58694f21c34878bad6ea66a95dbcfd","35c77e5f23054de9b2c04fa4439e460a","5e1f87a266d64f0f96a7fcad0a607e2d","150f1a4dc4144083bf8188f10d6ca0f6","e8af0fa9a35e4d10bcf5ee6364114bde","986a59f36731400ba8b690e1bf6e3d25","cda79a3250d54546b65d3eb666c6e209","b76bf3c174644fe1b0192c3ad06900a1","3648361cc89e475da9537e74002c5cdc","2361e53287834269a1a870745ced7cab","3bc72b8d0fad4efaa150bbefb233a3d3"]},"id":"bAWrE3Su15D3","executionInfo":{"status":"error","timestamp":1755834667153,"user_tz":-420,"elapsed":28818,"user":{"displayName":"VAP Solution","userId":"02613841423815522130"}},"outputId":"8203bec4-0310-4ee7-98a9-6fb3d6f52473"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Installing required packages...\n","All packages installed successfully!\n","================================================================================\n","IMPROVED NACC COMPLAINT CLASSIFICATION FINE-TUNING\n","================================================================================\n","\n","1. Setting up category mapping...\n","Category mapping saved to: /content/drive/MyDrive/Government/KPI/NACC AI Project/[00] NACC LLM/Paper Revision/finetune_new/category_mapping.json\n","\n","2. Loading and preprocessing data...\n","Loaded dataset with 800 samples\n","Columns: ['complaint', 'category']\n","\n","Data structure:\n","                                           complaint  \\\n","0  คณะผู้บริหารองค์การบริหารและสมาชิกสภาตำบลท่าดอ...   \n","1  จัดซื้อจัดจ้างโดยคัดเลือกจากผู้รับจ้างที่ผู้ถู...   \n","2  ผู้ถูกร้องใช้อำนาจหน้าที่กระทำการเอื้อประโยชน์...   \n","3  เบียดบังเงินของวัดโคกเข็ม รวมจำนวน ๓,๑๔๙,๙๙๙ บ...   \n","4  ดำเนินการขุดบ่อน้ำ เพื่อแก้ไขปัญหาภัยแล้งในเขต...   \n","\n","                                            category  \n","0  การขัดกันระหว่างประโยชน์ส่วนบุคคลกับประโยชน์ส่...  \n","1  การขัดกันระหว่างประโยชน์ส่วนบุคคลกับประโยชน์ส่...  \n","2  การขัดกันระหว่างประโยชน์ส่วนบุคคลกับประโยชน์ส่...  \n","3  การขัดกันระหว่างประโยชน์ส่วนบุคคลกับประโยชน์ส่...  \n","4  การขัดกันระหว่างประโยชน์ส่วนบุคคลกับประโยชน์ส่...  \n","\n","Dataset shape: (800, 2)\n","Warning: 8 unmapped categories found:\n","['ฝ่าฝืนจริยธรรม']\n","Final dataset size: 792\n","\n","Class distribution:\n","0: Abuse of Power - 298 samples (37.6%)\n","1: Budget/Project Fraud - 42 samples (5.3%)\n","2: Procurement Fraud - 227 samples (28.7%)\n","3: Fraudulent Land Title - 13 samples (1.6%)\n","4: Embezzlement - 82 samples (10.4%)\n","5: Personnel Misconduct - 28 samples (3.5%)\n","6: Unusual Wealth - 36 samples (4.5%)\n","7: Bribery - 54 samples (6.8%)\n","8: Conflict of Interest - 12 samples (1.5%)\n","\n","3. Implementing data augmentation...\n","Dataset is small, applying data augmentation...\n","Creating augmented data with ratio: 0.3\n","Generated 95 augmented samples\n","Original dataset: 792 samples\n","Augmented dataset: 887 samples\n","\n","4. Creating stratified splits...\n","Train set: 567 samples (63.9%)\n","Validation set: 142 samples (16.0%)\n","Test set: 178 samples (20.1%)\n","\n","Class distributions:\n","  Class 0: Train=214, Val=53, Test=67\n","  Class 1: Train=30, Val=8, Test=9\n","  Class 2: Train=164, Val=41, Test=52\n","  Class 3: Train=10, Val=2, Test=3\n","  Class 4: Train=60, Val=15, Test=19\n","  Class 5: Train=20, Val=5, Test=6\n","  Class 6: Train=26, Val=7, Test=8\n","  Class 7: Train=35, Val=9, Test=11\n","  Class 8: Train=8, Val=2, Test=3\n","  Class 9: Train=0, Val=0, Test=0\n","\n","5. Computing class weights...\n","Class weights:\n","  0 (Abuse of Power): 0.294\n","  1 (Budget/Project Fraud): 2.100\n","  2 (Procurement Fraud): 0.384\n","  3 (Fraudulent Land Title): 6.300\n","  4 (Embezzlement): 1.050\n","  5 (Personnel Misconduct): 3.150\n","  6 (Unusual Wealth): 2.423\n","  7 (Bribery): 1.800\n","  8 (Conflict of Interest): 7.875\n","\n","6. Creating datasets...\n","Datasets created successfully\n","\n","7. Setting up model and tokenizer...\n","Tokenizer loaded\n","4-bit quantization enabled\n","Error loading model with quantization: Using `bitsandbytes` 4-bit quantization requires the latest version of bitsandbytes: `pip install -U bitsandbytes`\n","Retrying without quantization...\n"]},{"output_type":"display_data","data":{"text/plain":["Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"84f6418b77c5474b8adeead02eedc065"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at scb10x/llama3.2-typhoon2-3b-instruct and are newly initialized: ['score.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"stream","name":"stdout","text":["Base model loaded\n","Small dataset detected. Applying additional regularization...\n","trainable params: 30,720 || all params: 3,224,968,192 || trainable%: 0.0010\n","\n","8. Tokenizing data...\n"]},{"output_type":"display_data","data":{"text/plain":["Map:   0%|          | 0/567 [00:00<?, ? examples/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"aada607319ba4409a67888bd28003837"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Map:   0%|          | 0/142 [00:00<?, ? examples/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"030e45549b91424c83ccd5a9927e7dc3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Map:   0%|          | 0/178 [00:00<?, ? examples/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4b58694f21c34878bad6ea66a95dbcfd"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Tokenization completed\n","\n","10. Setting up training...\n","Trainer created successfully\n","\n","11. Starting training...\n","==================================================\n"]},{"output_type":"error","ename":"TypeError","evalue":"AdvancedRegularizedTrainer.compute_loss() got an unexpected keyword argument 'num_items_in_batch'","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/tmp/ipython-input-4115770998.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    607\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    608\u001b[0m \u001b[0;31m# Train the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 609\u001b[0;31m \u001b[0mtraining_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    610\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    611\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Training completed!\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   2236\u001b[0m                 \u001b[0mhf_hub_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable_progress_bars\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2237\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2238\u001b[0;31m             return inner_training_loop(\n\u001b[0m\u001b[1;32m   2239\u001b[0m                 \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2240\u001b[0m                 \u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m_inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2580\u001b[0m                     )\n\u001b[1;32m   2581\u001b[0m                     \u001b[0;32mwith\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2582\u001b[0;31m                         \u001b[0mtr_loss_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_items_in_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2583\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2584\u001b[0m                     if (\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtraining_step\u001b[0;34m(self, model, inputs, num_items_in_batch)\u001b[0m\n\u001b[1;32m   3794\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3795\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_loss_context_manager\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3796\u001b[0;31m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_items_in_batch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_items_in_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3797\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3798\u001b[0m         \u001b[0;32mdel\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mTypeError\u001b[0m: AdvancedRegularizedTrainer.compute_loss() got an unexpected keyword argument 'num_items_in_batch'"]}]},{"cell_type":"code","source":["# Improved Fine-tuning Code for NACC Complaint Classification\n","\n","# Install required packages\n","import subprocess\n","import sys\n","\n","def install_package(package):\n","    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package])\n","\n","# Install required packages\n","print(\"Installing required packages...\")\n","try:\n","    install_package(\"bitsandbytes\")\n","    install_package(\"peft\")\n","    install_package(\"accelerate\")\n","    install_package(\"transformers>=4.35.0\")\n","    install_package(\"datasets\")\n","    install_package(\"scikit-learn\")\n","    install_package(\"matplotlib\")\n","    install_package(\"seaborn\")\n","    print(\"All packages installed successfully!\")\n","except Exception as e:\n","    print(f\"Error installing packages: {e}\")\n","\n","import os\n","import json\n","import pandas as pd\n","import numpy as np\n","import torch\n","from datetime import datetime\n","from sklearn.model_selection import train_test_split, StratifiedKFold\n","from sklearn.utils.class_weight import compute_class_weight\n","from sklearn.metrics import accuracy_score, f1_score, precision_recall_fscore_support, classification_report, confusion_matrix\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","\n","from datasets import Dataset, DatasetDict\n","from transformers import (\n","    AutoTokenizer,\n","    AutoModelForSequenceClassification,\n","    BitsAndBytesConfig,\n","    TrainingArguments,\n","    Trainer,\n","    EarlyStoppingCallback\n",")\n","from peft import get_peft_model, LoraConfig, TaskType\n","import warnings\n","warnings.filterwarnings('ignore')\n","\n","# =============================================================================\n","# CONFIGURATION\n","# =============================================================================\n","\n","# Paths\n","DATASET_PATH = \"/content/drive/MyDrive/Government/KPI/NACC AI Project/[00] NACC LLM/Paper Revision/data/Trainset.csv\"\n","OUTPUT_DIR = \"/content/drive/MyDrive/Government/KPI/NACC AI Project/[00] NACC LLM/Paper Revision/finetune_new\"\n","\n","# Model configuration\n","MODEL_ID = \"scb10x/llama3.2-typhoon2-3b-instruct\"\n","MAX_LENGTH = 512  # Shorter sequences for efficiency\n","BATCH_SIZE = 8    # Larger batch size\n","GRADIENT_ACCUMULATION = 2\n","NUM_EPOCHS = 5    # More epochs for gradual learning\n","LEARNING_RATE = 1e-5  # Even lower learning rate\n","LORA_R = 8        # Smaller rank to reduce overfitting\n","LORA_ALPHA = 16   # Adjusted alpha\n","LORA_DROPOUT = 0.2  # Higher dropout\n","WEIGHT_DECAY = 0.1  # Strong regularization\n","\n","# Data augmentation parameters\n","AUG_RATIO = 0.3   # 30% of data will be augmented\n","USE_AUGMENTATION = True\n","USE_CROSS_VALIDATION = True\n","CV_FOLDS = 5\n","\n","# Create output directory\n","os.makedirs(OUTPUT_DIR, exist_ok=True)\n","\n","print(\"=\" * 80)\n","print(\"IMPROVED NACC COMPLAINT CLASSIFICATION FINE-TUNING\")\n","print(\"=\" * 80)\n","\n","# =============================================================================\n","# 1. CATEGORY MAPPING\n","# =============================================================================\n","\n","print(\"\\n1. Setting up category mapping...\")\n","\n","CATEGORY_MAPPING = {\n","    \"ปฏิบัติหรือละเว้นการปฏิบัติหน้าที่โดยมิชอบ\": 0,\n","    \"ทุจริตในการจัดทำงบประมาณ/โครงการ/เบิกจ่ายเงินในโครงการเป็นเท็จ\": 1,\n","    \"จัดซื้อจัดจ้าง\": 2,\n","    \"ออกเอกสารสิทธิที่ดิน\": 3,\n","    \"ยักยอก/เบียดบังเงินหรือทรัพย์สินของราชการ\": 4,\n","    \"การบริหารงานบุคคล (การบรรจุ/แต่งตั้ง/เลื่อนตำแหน่ง/โยกย้าย/ลงโทษวินัย)\": 5,\n","    \"ร่ำรวยผิดปกติ\": 6,\n","    \"เรียกรับสินบน\": 7,\n","    \"การขัดกันระหว่างประโยชน์ส่วนบุคคลกับประโยชน์ส่วนรวม\": 8,\n","    \"ก่าเกื้นจริยธรรม\": 9\n","}\n","\n","NUMERIC_TO_CATEGORY = {v: k for k, v in CATEGORY_MAPPING.items()}\n","\n","ENGLISH_NAMES = {\n","    0: \"Abuse of Power\",\n","    1: \"Budget/Project Fraud\",\n","    2: \"Procurement Fraud\",\n","    3: \"Fraudulent Land Title\",\n","    4: \"Embezzlement\",\n","    5: \"Personnel Misconduct\",\n","    6: \"Unusual Wealth\",\n","    7: \"Bribery\",\n","    8: \"Conflict of Interest\",\n","    9: \"Ethical Misconduct\"\n","}\n","\n","# Save mapping\n","mapping_file = os.path.join(OUTPUT_DIR, \"category_mapping.json\")\n","with open(mapping_file, 'w', encoding='utf-8') as f:\n","    json.dump({\n","        \"thai_to_numeric\": CATEGORY_MAPPING,\n","        \"numeric_to_thai\": NUMERIC_TO_CATEGORY,\n","        \"english_names\": ENGLISH_NAMES\n","    }, f, ensure_ascii=False, indent=2)\n","\n","print(f\"Category mapping saved to: {mapping_file}\")\n","\n","# =============================================================================\n","# 2. DATA LOADING AND PREPROCESSING\n","# =============================================================================\n","\n","print(\"\\n2. Loading and preprocessing data...\")\n","\n","# Load data\n","try:\n","    df = pd.read_csv(DATASET_PATH, encoding='utf-8')\n","    print(f\"Loaded dataset with {len(df)} samples\")\n","    print(f\"Columns: {list(df.columns)}\")\n","except Exception as e:\n","    print(f\"Error loading dataset: {e}\")\n","    raise\n","\n","# Check data structure\n","print(\"\\nData structure:\")\n","print(df.head())\n","print(f\"\\nDataset shape: {df.shape}\")\n","\n","# Map categories to numeric\n","df['category_numeric'] = df['category'].map(CATEGORY_MAPPING)\n","\n","# Check for unmapped categories\n","unmapped = df[df['category_numeric'].isna()]\n","if len(unmapped) > 0:\n","    print(f\"Warning: {len(unmapped)} unmapped categories found:\")\n","    print(unmapped['category'].unique())\n","    df = df.dropna(subset=['category_numeric'])\n","\n","# Convert to int\n","df['category_numeric'] = df['category_numeric'].astype(int)\n","\n","print(f\"Final dataset size: {len(df)}\")\n","\n","# Extract features and labels\n","X = df['complaint'].values\n","y = df['category_numeric'].values\n","\n","# Check class distribution\n","class_dist = df['category_numeric'].value_counts().sort_index()\n","print(\"\\nClass distribution:\")\n","for cat_id, count in class_dist.items():\n","    english_name = ENGLISH_NAMES[cat_id]\n","    percentage = (count / len(df)) * 100\n","    print(f\"{cat_id}: {english_name} - {count} samples ({percentage:.1f}%)\")\n","\n","# =============================================================================\n","# 3. DATA AUGMENTATION FOR SMALL DATASET\n","# =============================================================================\n","\n","print(\"\\n3. Implementing data augmentation...\")\n","\n","def simple_back_translation_augment(text):\n","    \"\"\"Simple augmentation through word shuffling and synonym replacement\"\"\"\n","    import random\n","\n","    # Simple word shuffling (keeping sentence structure)\n","    sentences = text.split('.')\n","    augmented_sentences = []\n","\n","    for sentence in sentences:\n","        words = sentence.strip().split()\n","        if len(words) > 3:\n","            # Shuffle middle words occasionally\n","            if random.random() < 0.3:\n","                middle = words[1:-1]\n","                random.shuffle(middle)\n","                words = [words[0]] + middle + [words[-1]]\n","        augmented_sentences.append(' '.join(words))\n","\n","    return '.'.join(augmented_sentences)\n","\n","def character_level_augment(text):\n","    \"\"\"Character-level augmentation for Thai text\"\"\"\n","    import random\n","\n","    chars = list(text)\n","    if len(chars) > 10:\n","        # Random character swapping (very conservative)\n","        if random.random() < 0.1:\n","            idx1, idx2 = random.sample(range(1, len(chars)-1), 2)\n","            chars[idx1], chars[idx2] = chars[idx2], chars[idx1]\n","\n","    return ''.join(chars)\n","\n","def augment_text(text, method='shuffle'):\n","    \"\"\"Apply text augmentation\"\"\"\n","    if method == 'shuffle':\n","        return simple_back_translation_augment(text)\n","    elif method == 'char':\n","        return character_level_augment(text)\n","    else:\n","        return text\n","\n","def create_augmented_data(X, y, augmentation_ratio=0.3):\n","    \"\"\"Create augmented dataset\"\"\"\n","    print(f\"Creating augmented data with ratio: {augmentation_ratio}\")\n","\n","    # Calculate samples to augment per class\n","    unique_classes, class_counts = np.unique(y, return_counts=True)\n","\n","    X_aug = []\n","    y_aug = []\n","\n","    for class_id in unique_classes:\n","        class_indices = np.where(y == class_id)[0]\n","        class_texts = X[class_indices]\n","\n","        # Calculate how many samples to augment for this class\n","        current_count = len(class_indices)\n","        target_augment = int(current_count * augmentation_ratio)\n","\n","        # Randomly select samples to augment\n","        aug_indices = np.random.choice(len(class_texts), size=target_augment, replace=True)\n","\n","        for idx in aug_indices:\n","            original_text = class_texts[idx]\n","\n","            # Apply different augmentation methods\n","            aug_method = np.random.choice(['shuffle', 'char'], p=[0.7, 0.3])\n","            augmented_text = augment_text(original_text, method=aug_method)\n","\n","            # Only add if augmentation actually changed the text\n","            if augmented_text != original_text:\n","                X_aug.append(augmented_text)\n","                y_aug.append(class_id)\n","\n","    print(f\"Generated {len(X_aug)} augmented samples\")\n","    return np.array(X_aug), np.array(y_aug)\n","\n","# Apply data augmentation if enabled\n","if USE_AUGMENTATION and len(df) < 1000:  # Only for small datasets\n","    print(\"Dataset is small, applying data augmentation...\")\n","    X_aug, y_aug = create_augmented_data(X, y, AUG_RATIO)\n","\n","    # Combine original and augmented data\n","    X_combined = np.concatenate([X, X_aug])\n","    y_combined = np.concatenate([y, y_aug])\n","\n","    print(f\"Original dataset: {len(X)} samples\")\n","    print(f\"Augmented dataset: {len(X_combined)} samples\")\n","\n","    # Use combined data for training\n","    X, y = X_combined, y_combined\n","else:\n","    print(\"Skipping augmentation (dataset size sufficient or disabled)\")\n","\n","# =============================================================================\n","# 4. STRATIFIED TRAIN-TEST SPLIT WITH CROSS-VALIDATION\n","# =============================================================================\n","\n","print(\"\\n4. Creating stratified splits...\")\n","\n","# First, create a hold-out test set (20%)\n","X_temp, X_test, y_temp, y_test = train_test_split(\n","    X, y,\n","    test_size=0.2,\n","    stratify=y,\n","    random_state=42\n",")\n","\n","# Then split remaining data for train/validation\n","X_train, X_val, y_train, y_val = train_test_split(\n","    X_temp, y_temp,\n","    test_size=0.2,  # 20% of remaining 80% = 16% of total\n","    stratify=y_temp,\n","    random_state=42\n",")\n","\n","print(f\"Train set: {len(X_train)} samples ({len(X_train)/len(X)*100:.1f}%)\")\n","print(f\"Validation set: {len(X_val)} samples ({len(X_val)/len(X)*100:.1f}%)\")\n","print(f\"Test set: {len(X_test)} samples ({len(X_test)/len(X)*100:.1f}%)\")\n","\n","# Check distributions\n","train_dist = pd.Series(y_train).value_counts().sort_index()\n","val_dist = pd.Series(y_val).value_counts().sort_index()\n","test_dist = pd.Series(y_test).value_counts().sort_index()\n","\n","print(\"\\nClass distributions:\")\n","for cat_id in range(10):\n","    train_count = train_dist.get(cat_id, 0)\n","    val_count = val_dist.get(cat_id, 0)\n","    test_count = test_dist.get(cat_id, 0)\n","    print(f\"  Class {cat_id}: Train={train_count}, Val={val_count}, Test={test_count}\")\n","\n","# =============================================================================\n","# 5. COMPUTE CLASS WEIGHTS FOR IMBALANCED DATA\n","# =============================================================================\n","\n","print(\"\\n5. Computing class weights...\")\n","\n","class_weights = compute_class_weight(\n","    'balanced',\n","    classes=np.unique(y_train),\n","    y=y_train\n",")\n","\n","class_weight_dict = {i: weight for i, weight in enumerate(class_weights)}\n","print(\"Class weights:\")\n","for cat_id, weight in class_weight_dict.items():\n","    if cat_id < len(ENGLISH_NAMES):\n","        print(f\"  {cat_id} ({ENGLISH_NAMES[cat_id]}): {weight:.3f}\")\n","\n","# =============================================================================\n","# 6. CREATE DATASETS WITH VALIDATION\n","# =============================================================================\n","\n","print(\"\\n6. Creating datasets...\")\n","\n","# Create datasets\n","train_dataset = Dataset.from_dict({\n","    'text': X_train,\n","    'labels': y_train\n","})\n","\n","val_dataset = Dataset.from_dict({\n","    'text': X_val,\n","    'labels': y_val\n","})\n","\n","test_dataset = Dataset.from_dict({\n","    'text': X_test,\n","    'labels': y_test\n","})\n","\n","# Create DatasetDict\n","datasets = DatasetDict({\n","    'train': train_dataset,\n","    'validation': val_dataset,\n","    'test': test_dataset\n","})\n","\n","print(\"Datasets created successfully\")\n","\n","# =============================================================================\n","# 7. MODEL AND TOKENIZER SETUP\n","# =============================================================================\n","\n","print(\"\\n7. Setting up model and tokenizer...\")\n","\n","# Load tokenizer\n","tokenizer = AutoTokenizer.from_pretrained(MODEL_ID)\n","if tokenizer.pad_token is None:\n","    tokenizer.pad_token = tokenizer.eos_token\n","    tokenizer.pad_token_id = tokenizer.eos_token_id\n","\n","print(\"Tokenizer loaded\")\n","\n","# Setup quantization config with fallback\n","try:\n","    quantization_config = BitsAndBytesConfig(\n","        load_in_4bit=True,\n","        bnb_4bit_use_double_quant=True,\n","        bnb_4bit_compute_dtype=torch.bfloat16,\n","        bnb_4bit_quant_type=\"nf4\"\n","    )\n","    use_quantization = True\n","    print(\"4-bit quantization enabled\")\n","except Exception as e:\n","    print(f\"Quantization not available: {e}\")\n","    print(\"Loading model without quantization...\")\n","    quantization_config = None\n","    use_quantization = False\n","\n","# Load model for classification\n","model_kwargs = {\n","    \"num_labels\": 10,  # Number of categories\n","    \"device_map\": \"auto\",\n","    \"trust_remote_code\": True,\n","    \"pad_token_id\": tokenizer.pad_token_id  # Add pad_token_id to model config\n","}\n","\n","if use_quantization and quantization_config is not None:\n","    model_kwargs[\"quantization_config\"] = quantization_config\n","\n","try:\n","    model = AutoModelForSequenceClassification.from_pretrained(\n","        MODEL_ID,\n","        **model_kwargs\n","    )\n","except Exception as e:\n","    print(f\"Error loading model with quantization: {e}\")\n","    print(\"Retrying without quantization...\")\n","    # Fallback without quantization\n","    model = AutoModelForSequenceClassification.from_pretrained(\n","        MODEL_ID,\n","        num_labels=10,\n","        trust_remote_code=True,\n","        pad_token_id=tokenizer.pad_token_id  # Add pad_token_id here too\n","    )\n","\n","print(\"Base model loaded\")\n","\n","# Set pad_token_id in model config if not already set\n","if model.config.pad_token_id is None:\n","    model.config.pad_token_id = tokenizer.pad_token_id\n","    print(f\"Set model pad_token_id to: {model.config.pad_token_id}\")\n","\n","# Apply LoRA with stronger regularization for small datasets\n","peft_config = LoraConfig(\n","    r=LORA_R,           # Smaller rank reduces parameters\n","    lora_alpha=LORA_ALPHA,\n","    lora_dropout=LORA_DROPOUT,  # Higher dropout\n","    bias=\"none\",\n","    task_type=TaskType.SEQ_CLS,\n","    target_modules=[\"q_proj\", \"v_proj\", \"k_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"]\n",")\n","\n","model = get_peft_model(model, peft_config)\n","\n","# Freeze more layers for small dataset\n","if len(X_train) < 1000:\n","    print(\"Small dataset detected. Applying additional regularization...\")\n","\n","    # Freeze embedding layers\n","    for name, param in model.named_parameters():\n","        if 'embed' in name.lower():\n","            param.requires_grad = False\n","\n","    # Freeze some transformer layers (freeze bottom layers, train top layers)\n","    total_layers = len([n for n, p in model.named_parameters() if 'layers.' in n and 'weight' in n])\n","    layers_to_freeze = total_layers // 3  # Freeze bottom 1/3 of layers\n","\n","    for name, param in model.named_parameters():\n","        if 'layers.' in name:\n","            layer_num = int(name.split('layers.')[1].split('.')[0])\n","            if layer_num < layers_to_freeze:\n","                param.requires_grad = False\n","\n","model.print_trainable_parameters()\n","\n","# =============================================================================\n","# 8. TOKENIZATION\n","# =============================================================================\n","\n","print(\"\\n8. Tokenizing data...\")\n","\n","def tokenize_function(examples):\n","    \"\"\"Tokenize complaints for classification\"\"\"\n","    return tokenizer(\n","        examples['text'],\n","        truncation=True,\n","        padding=True,\n","        max_length=MAX_LENGTH,\n","        return_tensors=None\n","    )\n","\n","# Tokenize datasets\n","tokenized_datasets = datasets.map(\n","    tokenize_function,\n","    batched=True,\n","    remove_columns=['text']\n",")\n","\n","print(\"Tokenization completed\")\n","\n","# =============================================================================\n","# 9. EVALUATION METRICS\n","# =============================================================================\n","\n","def compute_metrics(eval_pred):\n","    \"\"\"Compute comprehensive evaluation metrics\"\"\"\n","    predictions, labels = eval_pred\n","    predictions = np.argmax(predictions, axis=1)\n","\n","    # Basic metrics\n","    accuracy = accuracy_score(labels, predictions)\n","\n","    # Per-class metrics\n","    precision, recall, f1, support = precision_recall_fscore_support(\n","        labels, predictions, average=None, zero_division=0\n","    )\n","\n","    # Macro and weighted averages\n","    f1_macro = f1_score(labels, predictions, average='macro', zero_division=0)\n","    f1_weighted = f1_score(labels, predictions, average='weighted', zero_division=0)\n","\n","    return {\n","        'accuracy': accuracy,\n","        'f1_macro': f1_macro,\n","        'f1_weighted': f1_weighted,\n","        'precision_macro': np.mean(precision),\n","        'recall_macro': np.mean(recall)\n","    }\n","\n","# =============================================================================\n","# 10. TRAINING SETUP\n","# =============================================================================\n","\n","print(\"\\n10. Setting up training...\")\n","\n","# Custom trainer with advanced regularization for small datasets\n","class AdvancedRegularizedTrainer(Trainer):\n","    def __init__(self, class_weights=None, mixup_alpha=0.2, label_smoothing=0.1, **kwargs):\n","        super().__init__(**kwargs)\n","        self.class_weights = class_weights\n","        self.mixup_alpha = mixup_alpha\n","        self.label_smoothing = label_smoothing\n","\n","    def compute_loss(self, model, inputs, return_outputs=False):\n","        labels = inputs.get(\"labels\")\n","        outputs = model(**inputs)\n","        logits = outputs.get('logits')\n","\n","        # Apply class weights\n","        if self.class_weights is not None:\n","            weight_tensor = torch.tensor(list(self.class_weights.values()),\n","                                       dtype=torch.float32, device=labels.device)\n","        else:\n","            weight_tensor = None\n","\n","        # Label smoothing for regularization\n","        if self.label_smoothing > 0:\n","            loss_fct = torch.nn.CrossEntropyLoss(\n","                weight=weight_tensor,\n","                label_smoothing=self.label_smoothing\n","            )\n","        else:\n","            loss_fct = torch.nn.CrossEntropyLoss(weight=weight_tensor)\n","\n","        loss = loss_fct(logits, labels)\n","\n","        # Add L2 regularization to LoRA parameters\n","        l2_reg = 0\n","        for name, param in model.named_parameters():\n","            if 'lora_' in name and param.requires_grad:\n","                l2_reg += torch.norm(param, p=2)\n","\n","        loss = loss + 0.01 * l2_reg  # L2 regularization coefficient\n","\n","        return (loss, outputs) if return_outputs else loss\n","\n","# Enhanced training arguments for small datasets\n","# Using eval_strategy instead of evaluation_strategy for compatibility\n","training_args = TrainingArguments(\n","    output_dir=os.path.join(OUTPUT_DIR, \"checkpoints\"),\n","    num_train_epochs=NUM_EPOCHS,\n","    per_device_train_batch_size=BATCH_SIZE,\n","    per_device_eval_batch_size=BATCH_SIZE,\n","    gradient_accumulation_steps=GRADIENT_ACCUMULATION,\n","    learning_rate=LEARNING_RATE,\n","    weight_decay=WEIGHT_DECAY,\n","    warmup_ratio=0.1,\n","    lr_scheduler_type=\"cosine\",\n","    logging_steps=10,\n","    eval_strategy=\"steps\",  # Changed from evaluation_strategy\n","    eval_steps=50,\n","    save_strategy=\"steps\",\n","    save_steps=50,\n","    save_total_limit=5,\n","    load_best_model_at_end=True,\n","    metric_for_best_model=\"eval_f1_macro\",\n","    greater_is_better=True,\n","    report_to=\"none\",\n","    dataloader_pin_memory=False,\n","    bf16=torch.cuda.is_available(),\n","    fp16=not torch.cuda.is_available(),\n","    optim=\"adamw_torch\",\n","    max_grad_norm=1.0,\n","    dataloader_drop_last=True,\n","    run_name=f\"nacc-classification-{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",")\n","\n","# Create trainer with advanced regularization\n","trainer = AdvancedRegularizedTrainer(\n","    model=model,\n","    args=training_args,\n","    train_dataset=tokenized_datasets[\"train\"],\n","    eval_dataset=tokenized_datasets[\"validation\"],  # Use validation set\n","    compute_metrics=compute_metrics,\n","    class_weights=class_weight_dict,\n","    mixup_alpha=0.2 if len(X_train) < 1000 else 0.0,  # Enable mixup for small datasets\n","    label_smoothing=0.1,  # Label smoothing for regularization\n","    callbacks=[\n","        EarlyStoppingCallback(early_stopping_patience=5, early_stopping_threshold=0.001)\n","    ]\n",")\n","\n","print(\"Trainer created successfully\")\n","\n","# =============================================================================\n","# 11. TRAINING\n","# =============================================================================\n","\n","print(\"\\n11. Starting training...\")\n","print(\"=\" * 50)\n","\n","# Train the model\n","training_result = trainer.train()\n","\n","print(\"Training completed!\")\n","print(f\"Final training loss: {training_result.training_loss:.4f}\")\n","\n","# =============================================================================\n","# 12. EVALUATION WITH FINAL TEST SET\n","# =============================================================================\n","\n","print(\"\\n12. Evaluating model on validation and test sets...\")\n","\n","# Evaluate on validation set (used during training)\n","val_results = trainer.evaluate(eval_dataset=tokenized_datasets[\"validation\"])\n","print(\"Validation Results:\")\n","for key, value in val_results.items():\n","    if key.startswith('eval_'):\n","        print(f\"  {key}: {value:.4f}\")\n","\n","# Final evaluation on hold-out test set\n","test_results = trainer.evaluate(eval_dataset=tokenized_datasets[\"test\"])\n","print(\"\\nFinal Test Results:\")\n","for key, value in test_results.items():\n","    if key.startswith('eval_'):\n","        print(f\"  {key}: {value:.4f}\")\n","\n","# Detailed predictions on test set\n","test_predictions = trainer.predict(tokenized_datasets[\"test\"])\n","y_pred = np.argmax(test_predictions.predictions, axis=1)\n","y_true = test_predictions.label_ids\n","\n","# Classification report\n","report = classification_report(\n","    y_true, y_pred,\n","    target_names=[ENGLISH_NAMES[i] for i in range(10)],\n","    output_dict=True,\n","    zero_division=0\n",")\n","\n","print(\"\\nDetailed Test Set Classification Report:\")\n","print(classification_report(\n","    y_true, y_pred,\n","    target_names=[ENGLISH_NAMES[i] for i in range(10)],\n","    zero_division=0\n","))\n","\n","# Confusion matrix\n","cm = confusion_matrix(y_true, y_pred)\n","\n","# =============================================================================\n","# 13. CROSS-VALIDATION FOR ROBUST EVALUATION (OPTIONAL)\n","# =============================================================================\n","\n","if USE_CROSS_VALIDATION and len(X_train) < 1000:\n","    print(\"\\n13. Performing cross-validation for robust evaluation...\")\n","\n","    # Use original training + validation data for CV\n","    X_cv = np.concatenate([X_train, X_val])\n","    y_cv = np.concatenate([y_train, y_val])\n","\n","    skf = StratifiedKFold(n_splits=CV_FOLDS, shuffle=True, random_state=42)\n","    cv_scores = []\n","\n","    for fold, (train_idx, val_idx) in enumerate(skf.split(X_cv, y_cv)):\n","        print(f\"Training fold {fold+1}/{CV_FOLDS}...\")\n","\n","        # Create fold datasets\n","        X_fold_train, X_fold_val = X_cv[train_idx], X_cv[val_idx]\n","        y_fold_train, y_fold_val = y_cv[train_idx], y_cv[val_idx]\n","\n","        # Create datasets for this fold\n","        fold_train_dataset = Dataset.from_dict({\n","            'text': X_fold_train,\n","            'labels': y_fold_train\n","        })\n","\n","        fold_val_dataset = Dataset.from_dict({\n","            'text': X_fold_val,\n","            'labels': y_fold_val\n","        })\n","\n","        # Tokenize\n","        fold_train_tokenized = fold_train_dataset.map(tokenize_function, batched=True, remove_columns=['text'])\n","        fold_val_tokenized = fold_val_dataset.map(tokenize_function, batched=True, remove_columns=['text'])\n","\n","        # Create new model for this fold (reset weights)\n","        fold_model = AutoModelForSequenceClassification.from_pretrained(\n","            MODEL_ID,\n","            num_labels=10,\n","            trust_remote_code=True,\n","            pad_token_id=tokenizer.pad_token_id  # Add pad_token_id\n","        )\n","        # Set pad_token_id in model config if not already set\n","        if fold_model.config.pad_token_id is None:\n","            fold_model.config.pad_token_id = tokenizer.pad_token_id\n","        fold_model = get_peft_model(fold_model, peft_config)\n","\n","        # Create trainer for this fold\n","        fold_training_args = TrainingArguments(\n","            output_dir=os.path.join(OUTPUT_DIR, f\"cv_fold_{fold}\"),\n","            num_train_epochs=NUM_EPOCHS,\n","            per_device_train_batch_size=BATCH_SIZE,\n","            per_device_eval_batch_size=BATCH_SIZE,\n","            learning_rate=LEARNING_RATE,\n","            weight_decay=WEIGHT_DECAY,\n","            eval_strategy=\"no\",  # Changed from evaluation_strategy\n","            save_strategy=\"no\",\n","            logging_steps=1000,\n","            report_to=\"none\",\n","            bf16=torch.cuda.is_available(),\n","        )\n","\n","        fold_trainer = AdvancedRegularizedTrainer(\n","            model=fold_model,\n","            args=fold_training_args,\n","            train_dataset=fold_train_tokenized,\n","            eval_dataset=fold_val_tokenized,\n","            compute_metrics=compute_metrics,\n","            class_weights=class_weight_dict\n","        )\n","\n","        # Train fold\n","        fold_trainer.train()\n","\n","        # Evaluate fold\n","        fold_results = fold_trainer.evaluate()\n","        cv_scores.append(fold_results['eval_f1_macro'])\n","\n","        print(f\"Fold {fold+1} F1-Macro: {fold_results['eval_f1_macro']:.4f}\")\n","\n","        # Clean up memory\n","        del fold_model, fold_trainer\n","        torch.cuda.empty_cache() if torch.cuda.is_available() else None\n","\n","    print(f\"\\nCross-validation results:\")\n","    print(f\"Mean F1-Macro: {np.mean(cv_scores):.4f} ± {np.std(cv_scores):.4f}\")\n","    print(f\"Individual folds: {cv_scores}\")\n","\n","    # Add CV results to final results\n","    cv_results = {\n","        \"cv_f1_macro_mean\": np.mean(cv_scores),\n","        \"cv_f1_macro_std\": np.std(cv_scores),\n","        \"cv_scores\": cv_scores\n","    }\n","else:\n","    print(\"\\n13. Skipping cross-validation\")\n","    cv_results = {}\n","\n","# =============================================================================\n","# 14. VISUALIZATION AND SAVING RESULTS\n","# =============================================================================\n","\n","print(\"\\n14. Saving results and visualizations...\")\n","\n","# Save comprehensive results\n","results = {\n","    \"training_args\": training_args.to_dict(),\n","    \"model_config\": {\n","        \"base_model\": MODEL_ID,\n","        \"max_length\": MAX_LENGTH,\n","        \"lora_config\": {\n","            \"r\": LORA_R,\n","            \"alpha\": LORA_ALPHA,\n","            \"dropout\": LORA_DROPOUT\n","        },\n","        \"regularization\": {\n","            \"weight_decay\": WEIGHT_DECAY,\n","            \"label_smoothing\": 0.1,\n","            \"gradient_clipping\": 1.0,\n","            \"layer_freezing\": len(X_train) < 1000\n","        }\n","    },\n","    \"dataset_info\": {\n","        \"total_samples\": len(X),\n","        \"train_samples\": len(X_train),\n","        \"val_samples\": len(X_val),\n","        \"test_samples\": len(X_test),\n","        \"augmentation_applied\": USE_AUGMENTATION,\n","        \"augmentation_ratio\": AUG_RATIO if USE_AUGMENTATION else 0\n","    },\n","    \"training_results\": {\n","        \"final_loss\": training_result.training_loss,\n","        \"train_steps\": training_result.global_step\n","    },\n","    \"validation_results\": val_results,\n","    \"test_results\": test_results,\n","    \"classification_report\": report,\n","    \"confusion_matrix\": cm.tolist(),\n","    \"class_distribution\": {\n","        \"train\": train_dist.to_dict(),\n","        \"validation\": val_dist.to_dict(),\n","        \"test\": test_dist.to_dict()\n","    },\n","    \"class_weights\": class_weight_dict,\n","    \"cross_validation\": cv_results,\n","    \"timestamp\": datetime.now().isoformat()\n","}\n","\n","# Save results\n","results_file = os.path.join(OUTPUT_DIR, \"training_results.json\")\n","with open(results_file, 'w', encoding='utf-8') as f:\n","    json.dump(results, f, ensure_ascii=False, indent=2)\n","\n","print(f\"Results saved to: {results_file}\")\n","\n","# Create confusion matrix plot\n","plt.figure(figsize=(12, 10))\n","sns.heatmap(\n","    cm,\n","    annot=True,\n","    fmt='d',\n","    cmap='Blues',\n","    xticklabels=[ENGLISH_NAMES[i] for i in range(10)],\n","    yticklabels=[ENGLISH_NAMES[i] for i in range(10)]\n",")\n","plt.title('Confusion Matrix - NACC Complaint Classification')\n","plt.xlabel('Predicted')\n","plt.ylabel('Actual')\n","plt.xticks(rotation=45, ha='right')\n","plt.yticks(rotation=0)\n","plt.tight_layout()\n","plt.savefig(os.path.join(OUTPUT_DIR, \"confusion_matrix.png\"), dpi=300, bbox_inches='tight')\n","plt.close()\n","\n","# Create class distribution plot\n","fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n","\n","# Train distribution\n","train_dist.plot(kind='bar', ax=ax1, color='skyblue')\n","ax1.set_title('Training Set Class Distribution')\n","ax1.set_xlabel('Category ID')\n","ax1.set_ylabel('Count')\n","ax1.tick_params(axis='x', rotation=0)\n","\n","# Test distribution\n","test_dist.plot(kind='bar', ax=ax2, color='lightcoral')\n","ax2.set_title('Test Set Class Distribution')\n","ax2.set_xlabel('Category ID')\n","ax2.set_ylabel('Count')\n","ax2.tick_params(axis='x', rotation=0)\n","\n","plt.tight_layout()\n","plt.savefig(os.path.join(OUTPUT_DIR, \"class_distribution.png\"), dpi=300, bbox_inches='tight')\n","plt.close()\n","\n","print(f\"Visualizations saved to: {OUTPUT_DIR}\")\n","\n","# =============================================================================\n","# 15. SAVE MODEL\n","# =============================================================================\n","\n","print(\"\\n15. Saving trained model...\")\n","\n","# Save model and tokenizer\n","model_save_path = os.path.join(OUTPUT_DIR, \"model\")\n","tokenizer_save_path = os.path.join(OUTPUT_DIR, \"tokenizer\")\n","\n","trainer.save_model(model_save_path)\n","tokenizer.save_pretrained(tokenizer_save_path)\n","\n","print(f\"Model saved to: {model_save_path}\")\n","print(f\"Tokenizer saved to: {tokenizer_save_path}\")\n","\n","# =============================================================================\n","# 16. SAVE TEST SET WITH PREDICTIONS\n","# =============================================================================\n","\n","print(\"\\n16. Saving test set with predictions for future evaluation...\")\n","\n","# Create test set with predictions\n","test_df = pd.DataFrame({\n","    'complaint': X_test,\n","    'true_category': y_true,\n","    'predicted_category': y_pred,\n","    'true_category_thai': [NUMERIC_TO_CATEGORY[cat] for cat in y_true],\n","    'predicted_category_thai': [NUMERIC_TO_CATEGORY[cat] for cat in y_pred],\n","    'true_category_english': [ENGLISH_NAMES[cat] for cat in y_true],\n","    'predicted_category_english': [ENGLISH_NAMES[cat] for cat in y_pred],\n","    'correct_prediction': y_true == y_pred\n","})\n","\n","test_csv_path = os.path.join(OUTPUT_DIR, \"test_set_with_predictions.csv\")\n","test_df.to_csv(test_csv_path, index=False, encoding='utf-8')\n","\n","print(f\"Test set with predictions saved to: {test_csv_path}\")\n","\n","# =============================================================================\n","# 17. FINAL SUMMARY\n","# =============================================================================\n","\n","print(\"\\n\" + \"=\" * 80)\n","print(\"TRAINING COMPLETED SUCCESSFULLY!\")\n","print(\"=\" * 80)\n","\n","print(f\"\\nKey Results:\")\n","print(f\"  Accuracy: {test_results['eval_accuracy']:.3f}\")\n","print(f\"  Macro F1: {test_results['eval_f1_macro']:.3f}\")\n","print(f\"  Weighted F1: {test_results['eval_f1_weighted']:.3f}\")\n","\n","print(f\"\\nFiles saved to: {OUTPUT_DIR}\")\n","print(f\"  - Model: {model_save_path}\")\n","print(f\"  - Tokenizer: {tokenizer_save_path}\")\n","print(f\"  - Results: {results_file}\")\n","print(f\"  - Test set: {test_csv_path}\")\n","print(f\"  - Confusion matrix: confusion_matrix.png\")\n","print(f\"  - Class distribution: class_distribution.png\")\n","\n","print(f\"\\nModel ready for deployment and evaluation!\")\n","\n","# =============================================================================\n","# 18. INFERENCE FUNCTION (FOR TESTING)\n","# =============================================================================\n","\n","def predict_complaint(text, model=model, tokenizer=tokenizer):\n","    \"\"\"\n","    Predict the category of a complaint text\n","\n","    Args:\n","        text: Input complaint text\n","        model: Trained model\n","        tokenizer: Tokenizer\n","\n","    Returns:\n","        Dictionary with prediction results\n","    \"\"\"\n","    # Tokenize input\n","    inputs = tokenizer(\n","        text,\n","        truncation=True,\n","        padding=True,\n","        max_length=MAX_LENGTH,\n","        return_tensors=\"pt\"\n","    )\n","\n","    # Move to device\n","    device = next(model.parameters()).device\n","    inputs = {k: v.to(device) for k, v in inputs.items()}\n","\n","    # Get predictions\n","    with torch.no_grad():\n","        outputs = model(**inputs)\n","        logits = outputs.logits\n","        probs = torch.softmax(logits, dim=-1)\n","\n","    # Get top prediction\n","    pred_idx = torch.argmax(probs, dim=-1).item()\n","    confidence = probs[0, pred_idx].item()\n","\n","    # Get top 3 predictions\n","    top3_probs, top3_indices = torch.topk(probs[0], k=min(3, len(probs[0])))\n","\n","    top3_predictions = []\n","    for prob, idx in zip(top3_probs, top3_indices):\n","        top3_predictions.append({\n","            \"category_id\": idx.item(),\n","            \"category_thai\": NUMERIC_TO_CATEGORY[idx.item()],\n","            \"category_english\": ENGLISH_NAMES[idx.item()],\n","            \"confidence\": prob.item()\n","        })\n","\n","    return {\n","        \"predicted_category_id\": pred_idx,\n","        \"predicted_category_thai\": NUMERIC_TO_CATEGORY[pred_idx],\n","        \"predicted_category_english\": ENGLISH_NAMES[pred_idx],\n","        \"confidence\": confidence,\n","        \"top3_predictions\": top3_predictions\n","    }\n","\n","# Test the inference function\n","print(\"\\n\" + \"=\" * 80)\n","print(\"TESTING INFERENCE FUNCTION\")\n","print(\"=\" * 80)\n","\n","# Example test\n","test_text = \"มีการเรียกรับเงินสินบนในการอนุมัติโครงการก่อสร้าง\"\n","print(f\"\\nTest text: {test_text}\")\n","\n","result = predict_complaint(test_text)\n","print(f\"\\nPrediction: {result['predicted_category_thai']}\")\n","print(f\"English: {result['predicted_category_english']}\")\n","print(f\"Confidence: {result['confidence']:.2%}\")\n","\n","print(\"\\nTop 3 predictions:\")\n","for i, pred in enumerate(result['top3_predictions'], 1):\n","    print(f\"  {i}. {pred['category_english']} ({pred['confidence']:.2%})\")\n","\n","print(\"\\n\" + \"=\" * 80)\n","print(\"ALL PROCESSES COMPLETED SUCCESSFULLY!\")\n","print(\"=\" * 80)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":373},"id":"OY4M38S17sc9","executionInfo":{"status":"error","timestamp":1756201116478,"user_tz":-420,"elapsed":21653,"user":{"displayName":"VAP Solution","userId":"02613841423815522130"}},"outputId":"5cbdca41-7571-4080-b7d0-2cadb5310170"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Installing required packages...\n"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipython-input-386619192.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0minstall_package\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"scikit-learn\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0minstall_package\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"matplotlib\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m     \u001b[0minstall_package\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"seaborn\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"All packages installed successfully!\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipython-input-386619192.py\u001b[0m in \u001b[0;36minstall_package\u001b[0;34m(package)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0minstall_package\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpackage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0msubprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecutable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"-m\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"pip\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"install\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpackage\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m# Install required packages\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.12/subprocess.py\u001b[0m in \u001b[0;36mcheck_call\u001b[0;34m(*popenargs, **kwargs)\u001b[0m\n\u001b[1;32m    406\u001b[0m     \u001b[0mcheck_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"ls\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"-l\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    407\u001b[0m     \"\"\"\n\u001b[0;32m--> 408\u001b[0;31m     \u001b[0mretcode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mpopenargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    409\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mretcode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    410\u001b[0m         \u001b[0mcmd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"args\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.12/subprocess.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(timeout, *popenargs, **kwargs)\u001b[0m\n\u001b[1;32m    389\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mPopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mpopenargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    390\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 391\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    392\u001b[0m         \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Including KeyboardInterrupt, wait handled that.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    393\u001b[0m             \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkill\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.12/subprocess.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1262\u001b[0m             \u001b[0mendtime\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_time\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1263\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1264\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1265\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1266\u001b[0m             \u001b[0;31m# https://bugs.python.org/issue25942\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.12/subprocess.py\u001b[0m in \u001b[0;36m_wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   2051\u001b[0m                         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturncode\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2052\u001b[0m                             \u001b[0;32mbreak\u001b[0m  \u001b[0;31m# Another thread waited.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2053\u001b[0;31m                         \u001b[0;34m(\u001b[0m\u001b[0mpid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msts\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2054\u001b[0m                         \u001b[0;31m# Check the pid and loop as waitpid has been known to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2055\u001b[0m                         \u001b[0;31m# return 0 even without WNOHANG in odd situations.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.12/subprocess.py\u001b[0m in \u001b[0;36m_try_wait\u001b[0;34m(self, wait_flags)\u001b[0m\n\u001b[1;32m   2009\u001b[0m             \u001b[0;34m\"\"\"All callers to this function MUST hold self._waitpid_lock.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2010\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2011\u001b[0;31m                 \u001b[0;34m(\u001b[0m\u001b[0mpid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msts\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwaitpid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwait_flags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2012\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mChildProcessError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2013\u001b[0m                 \u001b[0;31m# This happens if SIGCLD is set to be ignored or waiting\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"code","source":["# Improved Fine-tuning Code for NACC Complaint Classification\n","\n","# Install required packages\n","import subprocess\n","import sys\n","\n","def install_package(package):\n","    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package])\n","\n","# Install required packages\n","print(\"Installing required packages...\")\n","try:\n","    install_package(\"bitsandbytes\")\n","    install_package(\"peft\")\n","    install_package(\"accelerate\")\n","    install_package(\"transformers>=4.35.0\")\n","    install_package(\"datasets\")\n","    install_package(\"scikit-learn\")\n","    install_package(\"matplotlib\")\n","    install_package(\"seaborn\")\n","    print(\"All packages installed successfully!\")\n","except Exception as e:\n","    print(f\"Error installing packages: {e}\")\n","\n","import os\n","import json\n","import pandas as pd\n","import numpy as np\n","import torch\n","from datetime import datetime\n","from sklearn.model_selection import train_test_split, StratifiedKFold\n","from sklearn.utils.class_weight import compute_class_weight\n","from sklearn.metrics import accuracy_score, f1_score, precision_recall_fscore_support, classification_report, confusion_matrix\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","\n","from datasets import Dataset, DatasetDict\n","from transformers import (\n","    AutoTokenizer,\n","    AutoModelForSequenceClassification,\n","    BitsAndBytesConfig,\n","    TrainingArguments,\n","    Trainer,\n","    EarlyStoppingCallback\n",")\n","from peft import get_peft_model, LoraConfig, TaskType\n","import warnings\n","warnings.filterwarnings('ignore')\n","\n","# =============================================================================\n","# CONFIGURATION\n","# =============================================================================\n","\n","# Paths\n","DATASET_PATH = \"/content/drive/MyDrive/Government/KPI/NACC AI Project/[00] NACC LLM/Paper Revision/data/Trainset.csv\"\n","OUTPUT_DIR = \"/content/drive/MyDrive/Government/KPI/NACC AI Project/[00] NACC LLM/Paper Revision/finetune_new\"\n","\n","# Model configuration\n","MODEL_ID = \"scb10x/llama3.2-typhoon2-3b-instruct\"\n","MAX_LENGTH = 512  # Shorter sequences for efficiency\n","BATCH_SIZE = 8    # Larger batch size\n","GRADIENT_ACCUMULATION = 2\n","NUM_EPOCHS = 5    # More epochs for gradual learning\n","LEARNING_RATE = 1e-5  # Even lower learning rate\n","LORA_R = 8        # Smaller rank to reduce overfitting\n","LORA_ALPHA = 16   # Adjusted alpha\n","LORA_DROPOUT = 0.2  # Higher dropout\n","WEIGHT_DECAY = 0.1  # Strong regularization\n","\n","# Data augmentation parameters\n","AUG_RATIO = 0.3   # 30% of data will be augmented\n","USE_AUGMENTATION = True\n","USE_CROSS_VALIDATION = True\n","CV_FOLDS = 5\n","\n","# Create output directory\n","os.makedirs(OUTPUT_DIR, exist_ok=True)\n","\n","print(\"=\" * 80)\n","print(\"IMPROVED NACC COMPLAINT CLASSIFICATION FINE-TUNING\")\n","print(\"=\" * 80)\n","\n","# =============================================================================\n","# 1. CATEGORY MAPPING\n","# =============================================================================\n","\n","print(\"\\n1. Setting up category mapping...\")\n","\n","CATEGORY_MAPPING = {\n","    \"ปฏิบัติหรือละเว้นการปฏิบัติหน้าที่โดยมิชอบ\": 0,\n","    \"ทุจริตในการจัดทำงบประมาณ/โครงการ/เบิกจ่ายเงินในโครงการเป็นเท็จ\": 1,\n","    \"จัดซื้อจัดจ้าง\": 2,\n","    \"ออกเอกสารสิทธิที่ดิน\": 3,\n","    \"ยักยอก/เบียดบังเงินหรือทรัพย์สินของราชการ\": 4,\n","    \"การบริหารงานบุคคล (การบรรจุ/แต่งตั้ง/เลื่อนตำแหน่ง/โยกย้าย/ลงโทษวินัย)\": 5,\n","    \"ร่ำรวยผิดปกติ\": 6,\n","    \"เรียกรับสินบน\": 7,\n","    \"การขัดกันระหว่างประโยชน์ส่วนบุคคลกับประโยชน์ส่วนรวม\": 8,\n","    \"ก่าเกื้นจริยธรรม\": 9\n","}\n","\n","NUMERIC_TO_CATEGORY = {v: k for k, v in CATEGORY_MAPPING.items()}\n","\n","ENGLISH_NAMES = {\n","    0: \"Abuse of Power\",\n","    1: \"Budget/Project Fraud\",\n","    2: \"Procurement Fraud\",\n","    3: \"Fraudulent Land Title\",\n","    4: \"Embezzlement\",\n","    5: \"Personnel Misconduct\",\n","    6: \"Unusual Wealth\",\n","    7: \"Bribery\",\n","    8: \"Conflict of Interest\",\n","    9: \"Ethical Misconduct\"\n","}\n","\n","# Save mapping\n","mapping_file = os.path.join(OUTPUT_DIR, \"category_mapping.json\")\n","with open(mapping_file, 'w', encoding='utf-8') as f:\n","    json.dump({\n","        \"thai_to_numeric\": CATEGORY_MAPPING,\n","        \"numeric_to_thai\": NUMERIC_TO_CATEGORY,\n","        \"english_names\": ENGLISH_NAMES\n","    }, f, ensure_ascii=False, indent=2)\n","\n","print(f\"Category mapping saved to: {mapping_file}\")\n","\n","# =============================================================================\n","# 2. DATA LOADING AND PREPROCESSING\n","# =============================================================================\n","\n","print(\"\\n2. Loading and preprocessing data...\")\n","\n","# Load data\n","try:\n","    df = pd.read_csv(DATASET_PATH, encoding='utf-8')\n","    print(f\"Loaded dataset with {len(df)} samples\")\n","    print(f\"Columns: {list(df.columns)}\")\n","except Exception as e:\n","    print(f\"Error loading dataset: {e}\")\n","    raise\n","\n","# Check data structure\n","print(\"\\nData structure:\")\n","print(df.head())\n","print(f\"\\nDataset shape: {df.shape}\")\n","\n","# Map categories to numeric\n","df['category_numeric'] = df['category'].map(CATEGORY_MAPPING)\n","\n","# Check for unmapped categories\n","unmapped = df[df['category_numeric'].isna()]\n","if len(unmapped) > 0:\n","    print(f\"Warning: {len(unmapped)} unmapped categories found:\")\n","    print(unmapped['category'].unique())\n","    df = df.dropna(subset=['category_numeric'])\n","\n","# Convert to int\n","df['category_numeric'] = df['category_numeric'].astype(int)\n","\n","print(f\"Final dataset size: {len(df)}\")\n","\n","# Extract features and labels\n","X = df['complaint'].values\n","y = df['category_numeric'].values\n","\n","# Check class distribution\n","class_dist = df['category_numeric'].value_counts().sort_index()\n","print(\"\\nClass distribution:\")\n","for cat_id, count in class_dist.items():\n","    english_name = ENGLISH_NAMES[cat_id]\n","    percentage = (count / len(df)) * 100\n","    print(f\"{cat_id}: {english_name} - {count} samples ({percentage:.1f}%)\")\n","\n","# =============================================================================\n","# 3. DATA AUGMENTATION FOR SMALL DATASET\n","# =============================================================================\n","\n","print(\"\\n3. Implementing data augmentation...\")\n","\n","def simple_back_translation_augment(text):\n","    \"\"\"Simple augmentation through word shuffling and synonym replacement\"\"\"\n","    import random\n","\n","    # Simple word shuffling (keeping sentence structure)\n","    sentences = text.split('.')\n","    augmented_sentences = []\n","\n","    for sentence in sentences:\n","        words = sentence.strip().split()\n","        if len(words) > 3:\n","            # Shuffle middle words occasionally\n","            if random.random() < 0.3:\n","                middle = words[1:-1]\n","                random.shuffle(middle)\n","                words = [words[0]] + middle + [words[-1]]\n","        augmented_sentences.append(' '.join(words))\n","\n","    return '.'.join(augmented_sentences)\n","\n","def character_level_augment(text):\n","    \"\"\"Character-level augmentation for Thai text\"\"\"\n","    import random\n","\n","    chars = list(text)\n","    if len(chars) > 10:\n","        # Random character swapping (very conservative)\n","        if random.random() < 0.1:\n","            idx1, idx2 = random.sample(range(1, len(chars)-1), 2)\n","            chars[idx1], chars[idx2] = chars[idx2], chars[idx1]\n","\n","    return ''.join(chars)\n","\n","def augment_text(text, method='shuffle'):\n","    \"\"\"Apply text augmentation\"\"\"\n","    if method == 'shuffle':\n","        return simple_back_translation_augment(text)\n","    elif method == 'char':\n","        return character_level_augment(text)\n","    else:\n","        return text\n","\n","def create_augmented_data(X, y, augmentation_ratio=0.3):\n","    \"\"\"Create augmented dataset\"\"\"\n","    print(f\"Creating augmented data with ratio: {augmentation_ratio}\")\n","\n","    # Calculate samples to augment per class\n","    unique_classes, class_counts = np.unique(y, return_counts=True)\n","\n","    X_aug = []\n","    y_aug = []\n","\n","    for class_id in unique_classes:\n","        class_indices = np.where(y == class_id)[0]\n","        class_texts = X[class_indices]\n","\n","        # Calculate how many samples to augment for this class\n","        current_count = len(class_indices)\n","        target_augment = int(current_count * augmentation_ratio)\n","\n","        # Randomly select samples to augment\n","        aug_indices = np.random.choice(len(class_texts), size=target_augment, replace=True)\n","\n","        for idx in aug_indices:\n","            original_text = class_texts[idx]\n","\n","            # Apply different augmentation methods\n","            aug_method = np.random.choice(['shuffle', 'char'], p=[0.7, 0.3])\n","            augmented_text = augment_text(original_text, method=aug_method)\n","\n","            # Only add if augmentation actually changed the text\n","            if augmented_text != original_text:\n","                X_aug.append(augmented_text)\n","                y_aug.append(class_id)\n","\n","    print(f\"Generated {len(X_aug)} augmented samples\")\n","    return np.array(X_aug), np.array(y_aug)\n","\n","# Apply data augmentation if enabled\n","if USE_AUGMENTATION and len(df) < 1000:  # Only for small datasets\n","    print(\"Dataset is small, applying data augmentation...\")\n","    X_aug, y_aug = create_augmented_data(X, y, AUG_RATIO)\n","\n","    # Combine original and augmented data\n","    X_combined = np.concatenate([X, X_aug])\n","    y_combined = np.concatenate([y, y_aug])\n","\n","    print(f\"Original dataset: {len(X)} samples\")\n","    print(f\"Augmented dataset: {len(X_combined)} samples\")\n","\n","    # Use combined data for training\n","    X, y = X_combined, y_combined\n","else:\n","    print(\"Skipping augmentation (dataset size sufficient or disabled)\")\n","\n","# =============================================================================\n","# 4. STRATIFIED TRAIN-TEST SPLIT WITH CROSS-VALIDATION\n","# =============================================================================\n","\n","print(\"\\n4. Creating stratified splits...\")\n","\n","# First, create a hold-out test set (20%)\n","X_temp, X_test, y_temp, y_test = train_test_split(\n","    X, y,\n","    test_size=0.2,\n","    stratify=y,\n","    random_state=42\n",")\n","\n","# Then split remaining data for train/validation\n","X_train, X_val, y_train, y_val = train_test_split(\n","    X_temp, y_temp,\n","    test_size=0.2,  # 20% of remaining 80% = 16% of total\n","    stratify=y_temp,\n","    random_state=42\n",")\n","\n","print(f\"Train set: {len(X_train)} samples ({len(X_train)/len(X)*100:.1f}%)\")\n","print(f\"Validation set: {len(X_val)} samples ({len(X_val)/len(X)*100:.1f}%)\")\n","print(f\"Test set: {len(X_test)} samples ({len(X_test)/len(X)*100:.1f}%)\")\n","\n","# Check distributions\n","train_dist = pd.Series(y_train).value_counts().sort_index()\n","val_dist = pd.Series(y_val).value_counts().sort_index()\n","test_dist = pd.Series(y_test).value_counts().sort_index()\n","\n","print(\"\\nClass distributions:\")\n","for cat_id in range(10):\n","    train_count = train_dist.get(cat_id, 0)\n","    val_count = val_dist.get(cat_id, 0)\n","    test_count = test_dist.get(cat_id, 0)\n","    print(f\"  Class {cat_id}: Train={train_count}, Val={val_count}, Test={test_count}\")\n","\n","# =============================================================================\n","# 5. COMPUTE CLASS WEIGHTS FOR IMBALANCED DATA\n","# =============================================================================\n","\n","print(\"\\n5. Computing class weights...\")\n","\n","class_weights = compute_class_weight(\n","    'balanced',\n","    classes=np.unique(y_train),\n","    y=y_train\n",")\n","\n","class_weight_dict = {i: weight for i, weight in enumerate(class_weights)}\n","print(\"Class weights:\")\n","for cat_id, weight in class_weight_dict.items():\n","    if cat_id < len(ENGLISH_NAMES):\n","        print(f\"  {cat_id} ({ENGLISH_NAMES[cat_id]}): {weight:.3f}\")\n","\n","# =============================================================================\n","# 6. CREATE DATASETS WITH VALIDATION\n","# =============================================================================\n","\n","print(\"\\n6. Creating datasets...\")\n","\n","# Create datasets\n","train_dataset = Dataset.from_dict({\n","    'text': X_train,\n","    'labels': y_train\n","})\n","\n","val_dataset = Dataset.from_dict({\n","    'text': X_val,\n","    'labels': y_val\n","})\n","\n","test_dataset = Dataset.from_dict({\n","    'text': X_test,\n","    'labels': y_test\n","})\n","\n","# Create DatasetDict\n","datasets = DatasetDict({\n","    'train': train_dataset,\n","    'validation': val_dataset,\n","    'test': test_dataset\n","})\n","\n","print(\"Datasets created successfully\")\n","\n","# =============================================================================\n","# 7. MODEL AND TOKENIZER SETUP\n","# =============================================================================\n","\n","print(\"\\n7. Setting up model and tokenizer...\")\n","\n","# Load tokenizer\n","tokenizer = AutoTokenizer.from_pretrained(MODEL_ID)\n","\n","# Fix padding token properly for LLaMA-based models\n","if tokenizer.pad_token is None:\n","    # For LLaMA models, we need to add a new pad token, not use eos_token\n","    tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n","    print(\"Added [PAD] token to tokenizer\")\n","\n","print(f\"Tokenizer loaded - pad_token: {tokenizer.pad_token}, pad_token_id: {tokenizer.pad_token_id}\")\n","\n","# Setup quantization config with fallback\n","try:\n","    quantization_config = BitsAndBytesConfig(\n","        load_in_4bit=True,\n","        bnb_4bit_use_double_quant=True,\n","        bnb_4bit_compute_dtype=torch.bfloat16,\n","        bnb_4bit_quant_type=\"nf4\"\n","    )\n","    use_quantization = True\n","    print(\"4-bit quantization enabled\")\n","except Exception as e:\n","    print(f\"Quantization not available: {e}\")\n","    print(\"Loading model without quantization...\")\n","    quantization_config = None\n","    use_quantization = False\n","\n","# Load model for classification\n","model_kwargs = {\n","    \"num_labels\": 10,  # Number of categories\n","    \"device_map\": \"auto\",\n","    \"trust_remote_code\": True\n","}\n","\n","if use_quantization and quantization_config is not None:\n","    model_kwargs[\"quantization_config\"] = quantization_config\n","\n","try:\n","    model = AutoModelForSequenceClassification.from_pretrained(\n","        MODEL_ID,\n","        **model_kwargs\n","    )\n","except Exception as e:\n","    print(f\"Error loading model with quantization: {e}\")\n","    print(\"Retrying without quantization...\")\n","    # Fallback without quantization\n","    model = AutoModelForSequenceClassification.from_pretrained(\n","        MODEL_ID,\n","        num_labels=10,\n","        trust_remote_code=True\n","    )\n","\n","print(\"Base model loaded\")\n","\n","# CRITICAL: Resize token embeddings if we added new tokens\n","if len(tokenizer) > model.config.vocab_size:\n","    model.resize_token_embeddings(len(tokenizer))\n","    print(f\"Resized model embeddings to {len(tokenizer)} tokens\")\n","\n","# Set pad_token_id in model config\n","model.config.pad_token_id = tokenizer.pad_token_id\n","print(f\"Set model pad_token_id to: {model.config.pad_token_id}\")\n","\n","# Apply LoRA with stronger regularization for small datasets\n","peft_config = LoraConfig(\n","    r=LORA_R,           # Smaller rank reduces parameters\n","    lora_alpha=LORA_ALPHA,\n","    lora_dropout=LORA_DROPOUT,  # Higher dropout\n","    bias=\"none\",\n","    task_type=TaskType.SEQ_CLS,\n","    target_modules=[\"q_proj\", \"v_proj\", \"k_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"]\n",")\n","\n","model = get_peft_model(model, peft_config)\n","\n","# Freeze more layers for small dataset\n","if len(X_train) < 1000:\n","    print(\"Small dataset detected. Applying additional regularization...\")\n","\n","    # Freeze embedding layers\n","    for name, param in model.named_parameters():\n","        if 'embed' in name.lower():\n","            param.requires_grad = False\n","\n","    # Freeze some transformer layers (freeze bottom layers, train top layers)\n","    total_layers = len([n for n, p in model.named_parameters() if 'layers.' in n and 'weight' in n])\n","    layers_to_freeze = total_layers // 3  # Freeze bottom 1/3 of layers\n","\n","    for name, param in model.named_parameters():\n","        if 'layers.' in name:\n","            layer_num = int(name.split('layers.')[1].split('.')[0])\n","            if layer_num < layers_to_freeze:\n","                param.requires_grad = False\n","\n","model.print_trainable_parameters()\n","\n","# =============================================================================\n","# 8. TOKENIZATION\n","# =============================================================================\n","\n","print(\"\\n8. Tokenizing data...\")\n","\n","def tokenize_function(examples):\n","    \"\"\"Tokenize complaints for classification\"\"\"\n","    return tokenizer(\n","        examples['text'],\n","        truncation=True,\n","        padding='max_length',  # Use max_length padding instead of True\n","        max_length=MAX_LENGTH,\n","        return_tensors=None\n","    )\n","\n","# Tokenize datasets\n","tokenized_datasets = datasets.map(\n","    tokenize_function,\n","    batched=True,\n","    remove_columns=['text']\n",")\n","\n","print(\"Tokenization completed\")\n","\n","# =============================================================================\n","# 9. EVALUATION METRICS\n","# =============================================================================\n","\n","def compute_metrics(eval_pred):\n","    \"\"\"Compute comprehensive evaluation metrics\"\"\"\n","    predictions, labels = eval_pred\n","    predictions = np.argmax(predictions, axis=1)\n","\n","    # Basic metrics\n","    accuracy = accuracy_score(labels, predictions)\n","\n","    # Per-class metrics\n","    precision, recall, f1, support = precision_recall_fscore_support(\n","        labels, predictions, average=None, zero_division=0\n","    )\n","\n","    # Macro and weighted averages\n","    f1_macro = f1_score(labels, predictions, average='macro', zero_division=0)\n","    f1_weighted = f1_score(labels, predictions, average='weighted', zero_division=0)\n","\n","    return {\n","        'accuracy': accuracy,\n","        'f1_macro': f1_macro,\n","        'f1_weighted': f1_weighted,\n","        'precision_macro': np.mean(precision),\n","        'recall_macro': np.mean(recall)\n","    }\n","\n","# =============================================================================\n","# 10. TRAINING SETUP\n","# =============================================================================\n","\n","print(\"\\n10. Setting up training...\")\n","\n","# Custom trainer with advanced regularization for small datasets\n","class AdvancedRegularizedTrainer(Trainer):\n","    def __init__(self, class_weights=None, mixup_alpha=0.2, label_smoothing=0.1, **kwargs):\n","        super().__init__(**kwargs)\n","        self.class_weights = class_weights\n","        self.mixup_alpha = mixup_alpha\n","        self.label_smoothing = label_smoothing\n","\n","    def compute_loss(self, model, inputs, return_outputs=False):\n","        labels = inputs.get(\"labels\")\n","        outputs = model(**inputs)\n","        logits = outputs.get('logits')\n","\n","        # Apply class weights\n","        if self.class_weights is not None:\n","            weight_tensor = torch.tensor(list(self.class_weights.values()),\n","                                       dtype=torch.float32, device=labels.device)\n","        else:\n","            weight_tensor = None\n","\n","        # Label smoothing for regularization\n","        if self.label_smoothing > 0:\n","            loss_fct = torch.nn.CrossEntropyLoss(\n","                weight=weight_tensor,\n","                label_smoothing=self.label_smoothing\n","            )\n","        else:\n","            loss_fct = torch.nn.CrossEntropyLoss(weight=weight_tensor)\n","\n","        loss = loss_fct(logits, labels)\n","\n","        # Add L2 regularization to LoRA parameters\n","        l2_reg = 0\n","        for name, param in model.named_parameters():\n","            if 'lora_' in name and param.requires_grad:\n","                l2_reg += torch.norm(param, p=2)\n","\n","        loss = loss + 0.01 * l2_reg  # L2 regularization coefficient\n","\n","        return (loss, outputs) if return_outputs else loss\n","\n","# Enhanced training arguments for small datasets\n","# Using eval_strategy instead of evaluation_strategy for compatibility\n","training_args = TrainingArguments(\n","    output_dir=os.path.join(OUTPUT_DIR, \"checkpoints\"),\n","    num_train_epochs=NUM_EPOCHS,\n","    per_device_train_batch_size=BATCH_SIZE,\n","    per_device_eval_batch_size=BATCH_SIZE,\n","    gradient_accumulation_steps=GRADIENT_ACCUMULATION,\n","    learning_rate=LEARNING_RATE,\n","    weight_decay=WEIGHT_DECAY,\n","    warmup_ratio=0.1,\n","    lr_scheduler_type=\"cosine\",\n","    logging_steps=10,\n","    eval_strategy=\"steps\",  # Changed from evaluation_strategy\n","    eval_steps=50,\n","    save_strategy=\"steps\",\n","    save_steps=50,\n","    save_total_limit=5,\n","    load_best_model_at_end=True,\n","    metric_for_best_model=\"eval_f1_macro\",\n","    greater_is_better=True,\n","    report_to=\"none\",\n","    dataloader_pin_memory=False,\n","    bf16=torch.cuda.is_available(),\n","    fp16=not torch.cuda.is_available(),\n","    optim=\"adamw_torch\",\n","    max_grad_norm=1.0,\n","    dataloader_drop_last=True,\n","    run_name=f\"nacc-classification-{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",")\n","\n","# Create trainer with advanced regularization\n","trainer = AdvancedRegularizedTrainer(\n","    model=model,\n","    args=training_args,\n","    train_dataset=tokenized_datasets[\"train\"],\n","    eval_dataset=tokenized_datasets[\"validation\"],  # Use validation set\n","    compute_metrics=compute_metrics,\n","    class_weights=class_weight_dict,\n","    mixup_alpha=0.2 if len(X_train) < 1000 else 0.0,  # Enable mixup for small datasets\n","    label_smoothing=0.1,  # Label smoothing for regularization\n","    callbacks=[\n","        EarlyStoppingCallback(early_stopping_patience=5, early_stopping_threshold=0.001)\n","    ]\n",")\n","\n","print(\"Trainer created successfully\")\n","\n","# =============================================================================\n","# 11. TRAINING\n","# =============================================================================\n","\n","print(\"\\n11. Starting training...\")\n","print(\"=\" * 50)\n","\n","# Train the model\n","training_result = trainer.train()\n","\n","print(\"Training completed!\")\n","print(f\"Final training loss: {training_result.training_loss:.4f}\")\n","\n","# =============================================================================\n","# 12. EVALUATION WITH FINAL TEST SET\n","# =============================================================================\n","\n","print(\"\\n12. Evaluating model on validation and test sets...\")\n","\n","# Evaluate on validation set (used during training)\n","val_results = trainer.evaluate(eval_dataset=tokenized_datasets[\"validation\"])\n","print(\"Validation Results:\")\n","for key, value in val_results.items():\n","    if key.startswith('eval_'):\n","        print(f\"  {key}: {value:.4f}\")\n","\n","# Final evaluation on hold-out test set\n","test_results = trainer.evaluate(eval_dataset=tokenized_datasets[\"test\"])\n","print(\"\\nFinal Test Results:\")\n","for key, value in test_results.items():\n","    if key.startswith('eval_'):\n","        print(f\"  {key}: {value:.4f}\")\n","\n","# Detailed predictions on test set\n","test_predictions = trainer.predict(tokenized_datasets[\"test\"])\n","y_pred = np.argmax(test_predictions.predictions, axis=1)\n","y_true = test_predictions.label_ids\n","\n","# Classification report\n","report = classification_report(\n","    y_true, y_pred,\n","    target_names=[ENGLISH_NAMES[i] for i in range(10)],\n","    output_dict=True,\n","    zero_division=0\n",")\n","\n","print(\"\\nDetailed Test Set Classification Report:\")\n","print(classification_report(\n","    y_true, y_pred,\n","    target_names=[ENGLISH_NAMES[i] for i in range(10)],\n","    zero_division=0\n","))\n","\n","# Confusion matrix\n","cm = confusion_matrix(y_true, y_pred)\n","\n","# =============================================================================\n","# 13. CROSS-VALIDATION FOR ROBUST EVALUATION (OPTIONAL)\n","# =============================================================================\n","\n","if USE_CROSS_VALIDATION and len(X_train) < 1000:\n","    print(\"\\n13. Performing cross-validation for robust evaluation...\")\n","\n","    # Use original training + validation data for CV\n","    X_cv = np.concatenate([X_train, X_val])\n","    y_cv = np.concatenate([y_train, y_val])\n","\n","    skf = StratifiedKFold(n_splits=CV_FOLDS, shuffle=True, random_state=42)\n","    cv_scores = []\n","\n","    for fold, (train_idx, val_idx) in enumerate(skf.split(X_cv, y_cv)):\n","        print(f\"Training fold {fold+1}/{CV_FOLDS}...\")\n","\n","        # Create fold datasets\n","        X_fold_train, X_fold_val = X_cv[train_idx], X_cv[val_idx]\n","        y_fold_train, y_fold_val = y_cv[train_idx], y_cv[val_idx]\n","\n","        # Create datasets for this fold\n","        fold_train_dataset = Dataset.from_dict({\n","            'text': X_fold_train,\n","            'labels': y_fold_train\n","        })\n","\n","        fold_val_dataset = Dataset.from_dict({\n","            'text': X_fold_val,\n","            'labels': y_fold_val\n","        })\n","\n","        # Tokenize\n","        fold_train_tokenized = fold_train_dataset.map(tokenize_function, batched=True, remove_columns=['text'])\n","        fold_val_tokenized = fold_val_dataset.map(tokenize_function, batched=True, remove_columns=['text'])\n","\n","        # Create new model for this fold (reset weights)\n","        fold_model = AutoModelForSequenceClassification.from_pretrained(\n","            MODEL_ID,\n","            num_labels=10,\n","            trust_remote_code=True\n","        )\n","        # Resize token embeddings if needed\n","        if len(tokenizer) > fold_model.config.vocab_size:\n","            fold_model.resize_token_embeddings(len(tokenizer))\n","        # Set pad_token_id\n","        fold_model.config.pad_token_id = tokenizer.pad_token_id\n","        fold_model = get_peft_model(fold_model, peft_config)\n","\n","        # Create trainer for this fold\n","        fold_training_args = TrainingArguments(\n","            output_dir=os.path.join(OUTPUT_DIR, f\"cv_fold_{fold}\"),\n","            num_train_epochs=NUM_EPOCHS,\n","            per_device_train_batch_size=BATCH_SIZE,\n","            per_device_eval_batch_size=BATCH_SIZE,\n","            learning_rate=LEARNING_RATE,\n","            weight_decay=WEIGHT_DECAY,\n","            eval_strategy=\"no\",  # Changed from evaluation_strategy\n","            save_strategy=\"no\",\n","            logging_steps=1000,\n","            report_to=\"none\",\n","            bf16=torch.cuda.is_available(),\n","        )\n","\n","        fold_trainer = AdvancedRegularizedTrainer(\n","            model=fold_model,\n","            args=fold_training_args,\n","            train_dataset=fold_train_tokenized,\n","            eval_dataset=fold_val_tokenized,\n","            compute_metrics=compute_metrics,\n","            class_weights=class_weight_dict\n","        )\n","\n","        # Train fold\n","        fold_trainer.train()\n","\n","        # Evaluate fold\n","        fold_results = fold_trainer.evaluate()\n","        cv_scores.append(fold_results['eval_f1_macro'])\n","\n","        print(f\"Fold {fold+1} F1-Macro: {fold_results['eval_f1_macro']:.4f}\")\n","\n","        # Clean up memory\n","        del fold_model, fold_trainer\n","        torch.cuda.empty_cache() if torch.cuda.is_available() else None\n","\n","    print(f\"\\nCross-validation results:\")\n","    print(f\"Mean F1-Macro: {np.mean(cv_scores):.4f} ± {np.std(cv_scores):.4f}\")\n","    print(f\"Individual folds: {cv_scores}\")\n","\n","    # Add CV results to final results\n","    cv_results = {\n","        \"cv_f1_macro_mean\": np.mean(cv_scores),\n","        \"cv_f1_macro_std\": np.std(cv_scores),\n","        \"cv_scores\": cv_scores\n","    }\n","else:\n","    print(\"\\n13. Skipping cross-validation\")\n","    cv_results = {}\n","\n","# =============================================================================\n","# 14. VISUALIZATION AND SAVING RESULTS\n","# =============================================================================\n","\n","print(\"\\n14. Saving results and visualizations...\")\n","\n","# Save comprehensive results\n","results = {\n","    \"training_args\": training_args.to_dict(),\n","    \"model_config\": {\n","        \"base_model\": MODEL_ID,\n","        \"max_length\": MAX_LENGTH,\n","        \"lora_config\": {\n","            \"r\": LORA_R,\n","            \"alpha\": LORA_ALPHA,\n","            \"dropout\": LORA_DROPOUT\n","        },\n","        \"regularization\": {\n","            \"weight_decay\": WEIGHT_DECAY,\n","            \"label_smoothing\": 0.1,\n","            \"gradient_clipping\": 1.0,\n","            \"layer_freezing\": len(X_train) < 1000\n","        }\n","    },\n","    \"dataset_info\": {\n","        \"total_samples\": len(X),\n","        \"train_samples\": len(X_train),\n","        \"val_samples\": len(X_val),\n","        \"test_samples\": len(X_test),\n","        \"augmentation_applied\": USE_AUGMENTATION,\n","        \"augmentation_ratio\": AUG_RATIO if USE_AUGMENTATION else 0\n","    },\n","    \"training_results\": {\n","        \"final_loss\": training_result.training_loss,\n","        \"train_steps\": training_result.global_step\n","    },\n","    \"validation_results\": val_results,\n","    \"test_results\": test_results,\n","    \"classification_report\": report,\n","    \"confusion_matrix\": cm.tolist(),\n","    \"class_distribution\": {\n","        \"train\": train_dist.to_dict(),\n","        \"validation\": val_dist.to_dict(),\n","        \"test\": test_dist.to_dict()\n","    },\n","    \"class_weights\": class_weight_dict,\n","    \"cross_validation\": cv_results,\n","    \"timestamp\": datetime.now().isoformat()\n","}\n","\n","# Save results\n","results_file = os.path.join(OUTPUT_DIR, \"training_results.json\")\n","with open(results_file, 'w', encoding='utf-8') as f:\n","    json.dump(results, f, ensure_ascii=False, indent=2)\n","\n","print(f\"Results saved to: {results_file}\")\n","\n","# Create confusion matrix plot\n","plt.figure(figsize=(12, 10))\n","sns.heatmap(\n","    cm,\n","    annot=True,\n","    fmt='d',\n","    cmap='Blues',\n","    xticklabels=[ENGLISH_NAMES[i] for i in range(10)],\n","    yticklabels=[ENGLISH_NAMES[i] for i in range(10)]\n",")\n","plt.title('Confusion Matrix - NACC Complaint Classification')\n","plt.xlabel('Predicted')\n","plt.ylabel('Actual')\n","plt.xticks(rotation=45, ha='right')\n","plt.yticks(rotation=0)\n","plt.tight_layout()\n","plt.savefig(os.path.join(OUTPUT_DIR, \"confusion_matrix.png\"), dpi=300, bbox_inches='tight')\n","plt.close()\n","\n","# Create class distribution plot\n","fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n","\n","# Train distribution\n","train_dist.plot(kind='bar', ax=ax1, color='skyblue')\n","ax1.set_title('Training Set Class Distribution')\n","ax1.set_xlabel('Category ID')\n","ax1.set_ylabel('Count')\n","ax1.tick_params(axis='x', rotation=0)\n","\n","# Test distribution\n","test_dist.plot(kind='bar', ax=ax2, color='lightcoral')\n","ax2.set_title('Test Set Class Distribution')\n","ax2.set_xlabel('Category ID')\n","ax2.set_ylabel('Count')\n","ax2.tick_params(axis='x', rotation=0)\n","\n","plt.tight_layout()\n","plt.savefig(os.path.join(OUTPUT_DIR, \"class_distribution.png\"), dpi=300, bbox_inches='tight')\n","plt.close()\n","\n","print(f\"Visualizations saved to: {OUTPUT_DIR}\")\n","\n","# =============================================================================\n","# 15. SAVE MODEL\n","# =============================================================================\n","\n","print(\"\\n15. Saving trained model...\")\n","\n","# Save model and tokenizer\n","model_save_path = os.path.join(OUTPUT_DIR, \"model\")\n","tokenizer_save_path = os.path.join(OUTPUT_DIR, \"tokenizer\")\n","\n","trainer.save_model(model_save_path)\n","tokenizer.save_pretrained(tokenizer_save_path)\n","\n","print(f\"Model saved to: {model_save_path}\")\n","print(f\"Tokenizer saved to: {tokenizer_save_path}\")\n","\n","# =============================================================================\n","# 16. SAVE TEST SET WITH PREDICTIONS\n","# =============================================================================\n","\n","print(\"\\n16. Saving test set with predictions for future evaluation...\")\n","\n","# Create test set with predictions\n","test_df = pd.DataFrame({\n","    'complaint': X_test,\n","    'true_category': y_true,\n","    'predicted_category': y_pred,\n","    'true_category_thai': [NUMERIC_TO_CATEGORY[cat] for cat in y_true],\n","    'predicted_category_thai': [NUMERIC_TO_CATEGORY[cat] for cat in y_pred],\n","    'true_category_english': [ENGLISH_NAMES[cat] for cat in y_true],\n","    'predicted_category_english': [ENGLISH_NAMES[cat] for cat in y_pred],\n","    'correct_prediction': y_true == y_pred\n","})\n","\n","test_csv_path = os.path.join(OUTPUT_DIR, \"test_set_with_predictions.csv\")\n","test_df.to_csv(test_csv_path, index=False, encoding='utf-8')\n","\n","print(f\"Test set with predictions saved to: {test_csv_path}\")\n","\n","# =============================================================================\n","# 17. FINAL SUMMARY\n","# =============================================================================\n","\n","print(\"\\n\" + \"=\" * 80)\n","print(\"TRAINING COMPLETED SUCCESSFULLY!\")\n","print(\"=\" * 80)\n","\n","print(f\"\\nKey Results:\")\n","print(f\"  Accuracy: {test_results['eval_accuracy']:.3f}\")\n","print(f\"  Macro F1: {test_results['eval_f1_macro']:.3f}\")\n","print(f\"  Weighted F1: {test_results['eval_f1_weighted']:.3f}\")\n","\n","print(f\"\\nFiles saved to: {OUTPUT_DIR}\")\n","print(f\"  - Model: {model_save_path}\")\n","print(f\"  - Tokenizer: {tokenizer_save_path}\")\n","print(f\"  - Results: {results_file}\")\n","print(f\"  - Test set: {test_csv_path}\")\n","print(f\"  - Confusion matrix: confusion_matrix.png\")\n","print(f\"  - Class distribution: class_distribution.png\")\n","\n","print(f\"\\nModel ready for deployment and evaluation!\")\n","\n","# =============================================================================\n","# 18. INFERENCE FUNCTION (FOR TESTING)\n","# =============================================================================\n","\n","def predict_complaint(text, model=model, tokenizer=tokenizer):\n","    \"\"\"\n","    Predict the category of a complaint text\n","\n","    Args:\n","        text: Input complaint text\n","        model: Trained model\n","        tokenizer: Tokenizer\n","\n","    Returns:\n","        Dictionary with prediction results\n","    \"\"\"\n","    # Tokenize input\n","    inputs = tokenizer(\n","        text,\n","        truncation=True,\n","        padding=True,\n","        max_length=MAX_LENGTH,\n","        return_tensors=\"pt\"\n","    )\n","\n","    # Move to device\n","    device = next(model.parameters()).device\n","    inputs = {k: v.to(device) for k, v in inputs.items()}\n","\n","    # Get predictions\n","    with torch.no_grad():\n","        outputs = model(**inputs)\n","        logits = outputs.logits\n","        probs = torch.softmax(logits, dim=-1)\n","\n","    # Get top prediction\n","    pred_idx = torch.argmax(probs, dim=-1).item()\n","    confidence = probs[0, pred_idx].item()\n","\n","    # Get top 3 predictions\n","    top3_probs, top3_indices = torch.topk(probs[0], k=min(3, len(probs[0])))\n","\n","    top3_predictions = []\n","    for prob, idx in zip(top3_probs, top3_indices):\n","        top3_predictions.append({\n","            \"category_id\": idx.item(),\n","            \"category_thai\": NUMERIC_TO_CATEGORY[idx.item()],\n","            \"category_english\": ENGLISH_NAMES[idx.item()],\n","            \"confidence\": prob.item()\n","        })\n","\n","    return {\n","        \"predicted_category_id\": pred_idx,\n","        \"predicted_category_thai\": NUMERIC_TO_CATEGORY[pred_idx],\n","        \"predicted_category_english\": ENGLISH_NAMES[pred_idx],\n","        \"confidence\": confidence,\n","        \"top3_predictions\": top3_predictions\n","    }\n","\n","# Test the inference function\n","print(\"\\n\" + \"=\" * 80)\n","print(\"TESTING INFERENCE FUNCTION\")\n","print(\"=\" * 80)\n","\n","# Example test\n","test_text = \"มีการเรียกรับเงินสินบนในการอนุมัติโครงการก่อสร้าง\"\n","print(f\"\\nTest text: {test_text}\")\n","\n","result = predict_complaint(test_text)\n","print(f\"\\nPrediction: {result['predicted_category_thai']}\")\n","print(f\"English: {result['predicted_category_english']}\")\n","print(f\"Confidence: {result['confidence']:.2%}\")\n","\n","print(\"\\nTop 3 predictions:\")\n","for i, pred in enumerate(result['top3_predictions'], 1):\n","    print(f\"  {i}. {pred['category_english']} ({pred['confidence']:.2%})\")\n","\n","print(\"\\n\" + \"=\" * 80)\n","print(\"ALL PROCESSES COMPLETED SUCCESSFULLY!\")\n","print(\"=\" * 80)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":0},"id":"LEKRxUzIyAI1","executionInfo":{"status":"error","timestamp":1756201790965,"user_tz":-420,"elapsed":41296,"user":{"displayName":"VAP Solution","userId":"02613841423815522130"}},"outputId":"af7ceba2-3a64-495b-9987-a1295cba7f18"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Installing required packages...\n","All packages installed successfully!\n","================================================================================\n","IMPROVED NACC COMPLAINT CLASSIFICATION FINE-TUNING\n","================================================================================\n","\n","1. Setting up category mapping...\n","Category mapping saved to: /content/drive/MyDrive/Government/KPI/NACC AI Project/[00] NACC LLM/Paper Revision/finetune_new/category_mapping.json\n","\n","2. Loading and preprocessing data...\n","Error loading dataset: [Errno 2] No such file or directory: '/content/drive/MyDrive/Government/KPI/NACC AI Project/[00] NACC LLM/Paper Revision/data/Trainset.csv'\n"]},{"output_type":"error","ename":"FileNotFoundError","evalue":"[Errno 2] No such file or directory: '/content/drive/MyDrive/Government/KPI/NACC AI Project/[00] NACC LLM/Paper Revision/data/Trainset.csv'","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipython-input-3113245354.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[0;31m# Load data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 136\u001b[0;31m     \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDATASET_PATH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    137\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Loaded dataset with {len(df)} samples\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Columns: {list(df.columns)}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1619\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1620\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1622\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1879\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1880\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1881\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    880\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    881\u001b[0m             \u001b[0;31m# Binary mode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 882\u001b[0;31m             \u001b[0mhandle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    883\u001b[0m         \u001b[0mhandles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    884\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/drive/MyDrive/Government/KPI/NACC AI Project/[00] NACC LLM/Paper Revision/data/Trainset.csv'"]}]}]}